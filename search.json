[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text Analysis with R",
    "section": "",
    "text": "Authors: Renata Curty  and Jairo Melo \n\nThis three-part, hands-on workshop series introduces participants to the fundamentals of extracting insights from textual data using R. In Part 1: Text Preprocessing, we focus on cleaning and preparing text for analysis through normalization, noise reduction, stopword removal, tokenization, and lemmatization. Part 2 will delve into core text analysis techniques, including word frequencies, collocations, n-grams, and visualizations such as word clouds. Finally, Part 3 will explore sentiment analysis, applying polarity scoring and emotion detection methods. Throughout the series, we‚Äôll also highlight important caveats and best practices unique to working with textual data.\n\n\nThese lessons are hands-on and are designed to be followed with R and RStudio open. Before starting, please ensure you have the following software installed:\n\nR: We recommend R version 4.3 or newer. Download from CRAN.\nRStudio: We recommend RStudio version 2023.12 or newer. Download from Posit‚Äôs website.\n\n\n\n\n\n\n\nNoteHow to update R and RStudio\n\n\n\n\n\nCheck your versions\n\nRStudio:\n\nOn Mac: Go to RStudio -&gt; About RStudio.\nOn Windows: Go to Help -&gt; About RStudio.\n\nR: In the R console, run:\n\n#| eval: false\nR.version.string\nUpdate R\n\nGo to CRAN and download the latest version for your operating system.\nRun the installer. (You don‚Äôt need to uninstall older versions‚ÄîR will install alongside them.)\n\nUpdate RStudio\n\nGo to Posit‚Äôs download page.\nDownload and install the newest version for your operating system.\n\nThat‚Äôs it! After updating, restart your computer to make sure RStudio finds the latest R.\n\n\n\n\n\n\nFor this lesson we will analyze a dataset of social media posts related to the Apple TV series Severance. The dataset was collected using Brandwatch (via UCSB Library subscription), and it includes posts from the two days following the finales of Season 1 (April 2022) and Season 2 (March 2025). The dataset contains over 5,800 posts stored in a CSV file.\nThe R project containing the dataset and other files is available for download from this link: Severance Dataset. You will need an active UCSB NetID and password to access the file (the same you use for your UCSB email).\n\n\n\nThis level assumes a basic familiarity with R and RStudio. If you are new to R, we recommend you check out the Introduction to Data Analysis with R, particularly the Introduction to R and RStudio section.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Text Analysis with R",
    "section": "",
    "text": "These lessons are hands-on and are designed to be followed with R and RStudio open. Before starting, please ensure you have the following software installed:\n\nR: We recommend R version 4.3 or newer. Download from CRAN.\nRStudio: We recommend RStudio version 2023.12 or newer. Download from Posit‚Äôs website.\n\n\n\n\n\n\n\nNoteHow to update R and RStudio\n\n\n\n\n\nCheck your versions\n\nRStudio:\n\nOn Mac: Go to RStudio -&gt; About RStudio.\nOn Windows: Go to Help -&gt; About RStudio.\n\nR: In the R console, run:\n\n#| eval: false\nR.version.string\nUpdate R\n\nGo to CRAN and download the latest version for your operating system.\nRun the installer. (You don‚Äôt need to uninstall older versions‚ÄîR will install alongside them.)\n\nUpdate RStudio\n\nGo to Posit‚Äôs download page.\nDownload and install the newest version for your operating system.\n\nThat‚Äôs it! After updating, restart your computer to make sure RStudio finds the latest R.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#access-to-data",
    "href": "index.html#access-to-data",
    "title": "Text Analysis with R",
    "section": "",
    "text": "For this lesson we will analyze a dataset of social media posts related to the Apple TV series Severance. The dataset was collected using Brandwatch (via UCSB Library subscription), and it includes posts from the two days following the finales of Season 1 (April 2022) and Season 2 (March 2025). The dataset contains over 5,800 posts stored in a CSV file.\nThe R project containing the dataset and other files is available for download from this link: Severance Dataset. You will need an active UCSB NetID and password to access the file (the same you use for your UCSB email).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#r-skill-level",
    "href": "index.html#r-skill-level",
    "title": "Text Analysis with R",
    "section": "",
    "text": "This level assumes a basic familiarity with R and RStudio. If you are new to R, we recommend you check out the Introduction to Data Analysis with R, particularly the Introduction to R and RStudio section.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "chapters/3.SentimentAnalysis/introduction.html",
    "href": "chapters/3.SentimentAnalysis/introduction.html",
    "title": "Introduction to Sentiment Analysis",
    "section": "",
    "text": "Now that we have completed all the key preprocessing steps and our example dataset is in much better shape, we can finally proceed with sentiment analysis.",
    "crumbs": [
      "Sentiment Analysis",
      "Introduction to Sentiment Analysis"
    ]
  },
  {
    "objectID": "chapters/3.SentimentAnalysis/introduction.html#what-is-sentiment-analysis",
    "href": "chapters/3.SentimentAnalysis/introduction.html#what-is-sentiment-analysis",
    "title": "Introduction to Sentiment Analysis",
    "section": "What is Sentiment Analysis?",
    "text": "What is Sentiment Analysis?\nAs social beings, our beliefs, understanding of reality, and everyday decisions are deeply shaped by the opinions, perceptions and evaluations of others. This social conditioning is a well-documented phenomenon in fields such as psychology, sociology, and communication, where it is understood that individuals often rely on external cues, especially the attitudes and judgments of others when forming their own assessments.\nUnderstanding how public reaction and sentiment shapes and reflects collective perception has become central not only to corporate strategy, but also to scientific inquiry across many academic disciplines.\nWhile the analysis of public opinion predates the Internet, the modern field of sentiment analysis did not gain momentum until the mid-2000s. This surge was largely driven by the rise of Web 2.0, which leveraged the internet into a more participatory platform, enabling users to create, share, and comment on content across chats, blogs, forums, and other social media. These digital spaces dramatically expanded the circulation and accessibility of user-generated content, creating a fertile ground for computational approaches to analyze subjective expressions in large volumes of text. But what is sentiment analysis?\n\nSentiment analysis, also known as opinion mining, is now a well-established area of study within natural language processing (NLP) and computational linguistics. It focuses on identifying and extracting people‚Äôs opinions, evaluations, attitudes, and emotions from written language.\n\nWhether through product reviews, political commentary, or social media posts in virtually any possible topic of interest, sentiment analysis aims to quantify and interpret subjective information at scale, enabling applications in marketing, social science, finance, and beyond. In this course, we will explore ways of extracting insights from textual data, in particular how we can detect underlying emotions within messages shared by people on a popular streaming TV series.\nOur analysis pipeline will follow a two-step approach. First, we will compute basic sentiment polarity to determine whether viewers who commented on both season finales reacted more negatively, neutrally, or positively. Next, we will apply a more fine-grained emotion detection technique to capture and analyze the specific emotional expressions conveyed in the text.\nLet‚Äôs start by installing and loading the necessary packages, then bringing in the cleaned dataset so we can begin our sentiment analysis. We will discuss the role of each package in the next episodes.\n# Install packages (remove comments for packages you might have skipped)\ninstall.packages(\"sentimentr\")\ninstall.packages(\"syuzhet\")\n# install.packages(\"dplyr\")\n# install.packages(\"tidyr\")\n# install.packages(\"readr\")\n# install.packages(\"ggplot2\")\n# install.packages(\"RColorBrewer\")\n# install.packages(\"stringr\")\n\n# Load all packages\nlibrary(sentimentr)\nlibrary(syuzhet)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(stringr)\n\n# Load Data\ncomments &lt;- readr::read_csv(\"./data/comments_preprocessed.csv\")",
    "crumbs": [
      "Sentiment Analysis",
      "Introduction to Sentiment Analysis"
    ]
  },
  {
    "objectID": "chapters/3.SentimentAnalysis/considerations.html",
    "href": "chapters/3.SentimentAnalysis/considerations.html",
    "title": "Final Considerations",
    "section": "",
    "text": "Sentiment analysis, while a powerful method to extract insights from data, is far from perfect or straightforward. After all, it seeks to interpret natural language, which is constantly evolving. Speaking of evolution, did you know that the Cambridge dictionary added 6,000 words only this year, including ‚Äúbroligarchy‚Äù and ‚Äúdelulu‚Äù, many of which are widely used by Gen Alpha? This constant expansion of language highlights just how dynamic the texts we analyze can be.\nLanguage is also inherently rich, ambiguous, and culturally nuanced. Lexicon-based approaches, for instance, rely on predefined word lists and often struggle to capture subtleties in human expression.\nIn practice, sentiment analysis encounters issues like code-switching, where people mix languages in a single post, or compound sentences with mixed sentiments, such as ‚ÄúThe movie had great acting, but the ending was lame,‚Äù which are difficult to score accurately. Context dependence further complicates interpretation: words can flip polarity depending on the domain, like ‚Äúcheap,‚Äù which is positive when describing flights or monetary advantage in general, but negative when describing fabric or referring to quality.\nTemporal dynamics also play a role, as slang and cultural references evolve rapidly, e.g., ‚Äúbad‚Äù meaning ‚Äúgood‚Äù in some communities. Ambiguity adds another layer of difficulty: polysemous words like ‚Äúsick‚Äù can mean either ‚Äúill‚Äù or ‚Äúawesome‚Äù.\nAnother complication is to deal with sarcasm and irony which can completely invert the intended sentiment as in: ‚ÄúOh great, another awesome Monday morning traffic jam!‚Äù.\nImplicit sentiment may be present even when emotional words are absent, as in ‚ÄúThe waiter ignored us for 30 minutes before taking our order.‚Äù These factors collectively make sentiment analysis a useful but inherently imperfect tool for understanding human language and emotion.\nHowever, it is important to emphasize that as described before, we have only explored sentiment analysis through a lexicon-based approach, and that, as illustrated in Figure ? below, there are other methods, including machine learning, deep learning and their combination (hybrid), that can be employed to extract emotions from text, including user generated content, all with their own limitations and challenges.\nFor example, Amazon relies on deep learning algorithms to determine the sentiment of customer reviews by identifying positive, negative, or neutral tones in the text. The models are trained on a vast dataset of Amazon‚Äôs product descriptions and reviews and are regularly updated with new information. This robust approach enables Amazon to efficiently analyze and interpret customer feedback on a large scale.\nWhile there are more advanced approaches to sentiment analysis, including AI-assisted methods, these are discussion topics for future workshops!",
    "crumbs": [
      "Sentiment Analysis",
      "Final Considerations"
    ]
  },
  {
    "objectID": "chapters/3.SentimentAnalysis/considerations.html#references",
    "href": "chapters/3.SentimentAnalysis/considerations.html#references",
    "title": "Final Considerations",
    "section": "References",
    "text": "References\nMao, Y., Liu, Q., & Zhang, Y. (2024). Sentiment analysis methods, applications, and challenges: A systematic literature review. Journal of King Saud University - Computer and Information Sciences, 36(4), 102048. https://doi.org/10.1016/j.jksuci.2024.102048",
    "crumbs": [
      "Sentiment Analysis",
      "Final Considerations"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/tfidf.html",
    "href": "chapters/2.TextAnalysis/tfidf.html",
    "title": "TF-IDF: Finding Distinctive Vocabulary",
    "section": "",
    "text": "So far, we have explored word frequencies and n-grams to understand common terms and phrases in our text data. However, simply counting words has a limitation: some words are frequent because they appear often across all documents, not because they are particularly meaningful for a specific document or group.\nFor example, in our Severance dataset, words like ‚Äúseason,‚Äù ‚Äúepisode,‚Äù and ‚Äúshow‚Äù might appear frequently in comments about both Season 1 and Season 2. While these words are common, they don‚Äôt help us understand what makes each season‚Äôs discussion distinctive.\nThis is where TF-IDF (Term Frequency-Inverse Document Frequency) becomes useful. TF-IDF is a statistical measure that evaluates how important a word is to a document in a collection of documents (corpus). It helps us identify words that are frequent in one document but rare across the entire corpus‚Äîprecisely the words that make a document unique.",
    "crumbs": [
      "Text Analysis",
      "Frequency Analysis"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/tfidf.html#understanding-tf-idf",
    "href": "chapters/2.TextAnalysis/tfidf.html#understanding-tf-idf",
    "title": "TF-IDF: Finding Distinctive Vocabulary",
    "section": "Understanding TF-IDF",
    "text": "Understanding TF-IDF\nTF-IDF combines two metrics:\n\nTerm Frequency (TF): How often a word appears in a document\nInverse Document Frequency (IDF): How rare a word is across all documents\n\nThe formula is:\n\\[\\text{TF-IDF} = \\text{TF} \\times \\text{IDF}\\]\nWhere:\n\\[\\text{TF}(t, d) = \\frac{\\text{count of term } t \\text{ in document } d}{\\text{total terms in document } d}\\]\n\\[\\text{IDF}(t) = \\log\\left(\\frac{\\text{total number of documents}}{\\text{number of documents containing term } t}\\right)\\]\nA word gets a high TF-IDF score when:\n\nIt appears frequently in a particular document (high TF)\nIt appears in few other documents (high IDF)\n\nA word gets a low TF-IDF score when:\n\nIt appears in many documents (low IDF), even if it‚Äôs frequent in one document",
    "crumbs": [
      "Text Analysis",
      "Frequency Analysis"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/tfidf.html#calculating-tf-idf",
    "href": "chapters/2.TextAnalysis/tfidf.html#calculating-tf-idf",
    "title": "TF-IDF: Finding Distinctive Vocabulary",
    "section": "Calculating TF-IDF",
    "text": "Calculating TF-IDF\nIn our case, we want to compare the vocabulary between Season 1 and Season 2 comments. We‚Äôll treat each season as a ‚Äúdocument‚Äù and calculate TF-IDF to find which words are distinctive to each season.\nFirst, we need to extract season information from the id column and tokenize the comments:\n\n# Calculate TF-IDF by season\ncomments_tfidf &lt;- comments %&gt;%\n  mutate(season = str_extract(id, \"s[12]\")) %&gt;%  # Extract season (s1 or s2)\n  unnest_tokens(word, comments) %&gt;%               # Tokenize into words\n  count(season, word, sort = TRUE)                # Count words per season\n\nhead(comments_tfidf)\n\n# A tibble: 6 √ó 3\n  season word          n\n  &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;\n1 s2     severance  4200\n2 s2     season     3219\n3 s1     severance  1840\n4 s2     finale     1833\n5 s1     season     1261\n6 s2     show       1020\n\n\nNow we can apply the bind_tf_idf() function from the tidytext package, which automatically calculates TF, IDF, and TF-IDF for us:\n\n# Apply TF-IDF calculation\ncomments_tfidf &lt;- comments_tfidf %&gt;%\n  bind_tf_idf(word, season, n)\n\nhead(comments_tfidf, 15)\n\n# A tibble: 15 √ó 6\n   season word          n      tf   idf tf_idf\n   &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 s2     severance  4200 0.0809      0      0\n 2 s2     season     3219 0.0620      0      0\n 3 s1     severance  1840 0.0876      0      0\n 4 s2     finale     1833 0.0353      0      0\n 5 s1     season     1261 0.0601      0      0\n 6 s2     show       1020 0.0196      0      0\n 7 s1     finale      787 0.0375      0      0\n 8 s2     tv          658 0.0127      0      0\n 9 s1     show        632 0.0301      0      0\n10 s2     apple       586 0.0113      0      0\n11 s2     just        431 0.00830     0      0\n12 s1     tv          402 0.0191      0      0\n13 s2     like        354 0.00681     0      0\n14 s2     can         321 0.00618     0      0\n15 s1     apple       304 0.0145      0      0\n\n\nThe resulting data frame includes:\n\ntf: Term frequency (proportion of times the word appears in that season)\nidf: Inverse document frequency (how rare the word is across seasons)\ntf_idf: The product of TF and IDF\n\nLet‚Äôs examine the top words by TF-IDF for each season:\n\n# Top 10 distinctive words per season\ncomments_tfidf %&gt;%\n  group_by(season) %&gt;%\n  slice_max(tf_idf, n = 10)\n\n# A tibble: 20 √ó 6\n# Groups:   season [2]\n   season word             n       tf   idf   tf_idf\n   &lt;chr&gt;  &lt;chr&gt;        &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 s1     wonderfully     12 0.000571 0.693 0.000396\n 2 s1     tvtime          10 0.000476 0.693 0.000330\n 3 s1     crashed          8 0.000381 0.693 0.000264\n 4 s1     grey             8 0.000381 0.693 0.000264\n 5 s1     mets             8 0.000381 0.693 0.000264\n 6 s1     ptolemy          8 0.000381 0.693 0.000264\n 7 s1     captivating      6 0.000286 0.693 0.000198\n 8 s1     dga              6 0.000286 0.693 0.000198\n 9 s1     exhilarating     6 0.000286 0.693 0.000198\n10 s1     held             5 0.000238 0.693 0.000165\n11 s2     formally        84 0.00162  0.693 0.00112 \n12 s2     records         81 0.00156  0.693 0.00108 \n13 s2     cold            72 0.00139  0.693 0.000961\n14 s2     harbor          68 0.00131  0.693 0.000907\n15 s2     band            60 0.00116  0.693 0.000801\n16 s2     marching        51 0.000982 0.693 0.000681\n17 s2     march           45 0.000866 0.693 0.000600\n18 s2     silo            41 0.000789 0.693 0.000547\n19 s2     choice          39 0.000751 0.693 0.000520\n20 s2     voting          37 0.000712 0.693 0.000494\n\n\nNotice how these words are much more specific and meaningful than simply looking at the most frequent words. These are the words that truly characterize each season‚Äôs discussion.",
    "crumbs": [
      "Text Analysis",
      "Frequency Analysis"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/tfidf.html#visualizing-distinctive-vocabulary",
    "href": "chapters/2.TextAnalysis/tfidf.html#visualizing-distinctive-vocabulary",
    "title": "TF-IDF: Finding Distinctive Vocabulary",
    "section": "Visualizing Distinctive Vocabulary",
    "text": "Visualizing Distinctive Vocabulary\nTo better understand the distinctive vocabulary of each season, we can create a visualization comparing the top TF-IDF words:\n\n# Prepare data for visualization\ntop_tfidf_words &lt;- comments_tfidf %&gt;%\n  group_by(season) %&gt;%\n  slice_max(tf_idf, n = 15) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder_within(word, tf_idf, season))\n\n# Plot distinctive vocabulary by season\nggplot(top_tfidf_words, aes(tf_idf, word, fill = season)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~season, scales = \"free\") +\n  scale_y_reordered() +\n  labs(\n    x = \"TF-IDF\",\n    y = NULL,\n    title = \"Distinctive Vocabulary by Season\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis visualization clearly shows which words are most characteristic of each season‚Äôs discussions. Words with higher TF-IDF scores are those that appear frequently in one season but not in the other, making them useful markers of distinctive content.",
    "crumbs": [
      "Text Analysis",
      "Frequency Analysis"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/tfidf.html#comparing-tf-idf-to-raw-frequency",
    "href": "chapters/2.TextAnalysis/tfidf.html#comparing-tf-idf-to-raw-frequency",
    "title": "TF-IDF: Finding Distinctive Vocabulary",
    "section": "Comparing TF-IDF to Raw Frequency",
    "text": "Comparing TF-IDF to Raw Frequency\nTo appreciate the value of TF-IDF, let‚Äôs compare it to simple word counts. We‚Äôll look at the top words by frequency versus the top words by TF-IDF for Season 1:\n\n# Top words by raw frequency for Season 1\ntop_freq_s1 &lt;- comments %&gt;%\n  filter(grepl(\"^s1\", id)) %&gt;%\n  unnest_tokens(word, comments) %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  head(15)\n\n# Top words by TF-IDF for Season 1\ntop_tfidf_s1 &lt;- comments_tfidf %&gt;%\n  filter(season == \"s1\") %&gt;%\n  arrange(desc(tf_idf)) %&gt;%\n  head(15)\n\n# Top 15 words by frequency (Season 1)\nprint(top_freq_s1)\n\n# A tibble: 15 √ó 2\n   word          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 severance  1840\n 2 season     1261\n 3 finale      787\n 4 show        632\n 5 tv          402\n 6 apple       304\n 7 best        276\n 8 can         220\n 9 just        208\n10 wait        203\n11 one         193\n12 watch       191\n13 now         153\n14 good        148\n15 seen        142\n\n# Top 15 words by TF-IDF (Season 1)\nprint(top_tfidf_s1 %&gt;% select(word, n, tf_idf))\n\n# A tibble: 15 √ó 3\n   word             n   tf_idf\n   &lt;chr&gt;        &lt;int&gt;    &lt;dbl&gt;\n 1 wonderfully     12 0.000396\n 2 tvtime          10 0.000330\n 3 crashed          8 0.000264\n 4 grey             8 0.000264\n 5 mets             8 0.000264\n 6 ptolemy          8 0.000264\n 7 captivating      6 0.000198\n 8 dga              6 0.000198\n 9 exhilarating     6 0.000198\n10 held             5 0.000165\n11 alpha            4 0.000132\n12 computers        4 0.000132\n13 gary             4 0.000132\n14 mesmerized       4 0.000132\n15 moon             4 0.000132\n\n\nThe raw frequency list likely includes many words that are common across both seasons, while the TF-IDF list highlights words that are specifically important to Season 1 discussions.",
    "crumbs": [
      "Text Analysis",
      "Frequency Analysis"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/tfidf.html#when-to-use-tf-idf",
    "href": "chapters/2.TextAnalysis/tfidf.html#when-to-use-tf-idf",
    "title": "TF-IDF: Finding Distinctive Vocabulary",
    "section": "When to Use TF-IDF",
    "text": "When to Use TF-IDF\nTF-IDF is particularly useful for:\n\nDocument comparison: Identifying what makes each document unique in a collection\nFeature extraction: Preparing text data for machine learning by emphasizing distinctive words\nTopic discovery: Finding characteristic vocabulary for different groups or categories\nSearch and retrieval: Ranking documents by relevance to a query (search engines use variations of TF-IDF)\n\n\n\n\n\n\n\nTipLimitations of TF-IDF\n\n\n\nWhile TF-IDF is powerful, it has some limitations:\n\nNo semantic understanding: It treats words as independent units and doesn‚Äôt understand synonyms or context\nCorpus dependency: TF-IDF scores depend on the entire corpus, so adding or removing documents changes the scores\nDocument length bias: Can be affected by document length differences (though this is partially addressed by normalization)\n\nFor more advanced semantic analysis, techniques like word embeddings or transformer models might be more appropriate.\n\n\nTF-IDF bridges the gap between simple word counting and more sophisticated text analysis techniques. By weighing words based on both their local importance (in a document) and their global rarity (across the corpus), it helps us discover the vocabulary that truly distinguishes different parts of our text data.",
    "crumbs": [
      "Text Analysis",
      "Frequency Analysis"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/introduction.html",
    "href": "chapters/2.TextAnalysis/introduction.html",
    "title": "What is Text Analysis?",
    "section": "",
    "text": "Text analysis is an umbrella concept that involves multiple techniques, methods, and approaches for ‚Äúextracting‚Äù the meaning, structure, or general characteristics of a text by analyzing its constitutive words and symbols, and their relationships with a context, epoch, trend, intention, etc.\nThanks to the massification of computers and the miniaturization of computer power, computational methods for text analysis have become prevalent in certain contexts, allowing researchers to analyze large corpora of texts and also extrapolate those concepts for purposes beyond academic research, such as commercial text processing, sentiment analysis, or information retrieval.\nBuilding on these foundations, this episode focuses on the introductory analytical techniques that establish common ground for more complex tasks such as sentiment analysis, language modeling, topic modeling, or text generation.\n\n\n\n\n\n\nNoteNLP\n\n\n\nAlthough Natural Language Processing (NLP) is sometimes used as a synonym for text analysis, Text Analysis encompasses both computational and non-computational approaches to analyzing text. NLP is primarily concerned with the interaction between computers and human language. It focuses on developing algorithms and models that enable machines to understand, interpret, and generate human language.",
    "crumbs": [
      "Text Analysis",
      "Text Analysis"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/05_lemmatization.html",
    "href": "chapters/1.Preprocessing/05_lemmatization.html",
    "title": "Lemmatization",
    "section": "",
    "text": "Also known as word reduction, lemmatization is the process of transforming words into their base or dictionary form (lemma) to identify similarities in meaning and usage across different contexts. Take the word ‚Äúrun‚Äù as an example. It can appear in various forms like ‚Äúran‚Äù, ‚Äúruns‚Äù, ‚Äúrunning‚Äù, and ‚Äúrunner‚Äù. But the variations don‚Äôt stop there, as it includes complex forms like ‚Äúoutrun‚Äù, ‚Äúoverrun‚Äù, or ‚Äúunderrun‚Äù. These variations make it challenging for computers to process natural language effectively unless they can recognize that these words are related. That‚Äôs where lemmatization comes in; it helps machines group related words together by reducing them to a common root present in the dictionary, enabling better understanding and analysis of text.\nYou might be wondering: what about stemming? Stemming is a simpler but more aggressive process that removes prefixes and suffixes to reduce words to their root forms. It is generally considered less precise than lemmatization because it can sometimes produce meaningless words. For example, while ‚Äúran‚Äù and ‚Äúruns‚Äù would correctly stem to ‚Äúrun,‚Äù the word ‚Äúrunning‚Äù would be reduced to ‚Äúrunn.‚Äù\nFor this reason, we will stick with lemmatization and skip stemming in our pipeline. That said, if you need to process very large volumes of text and want a faster, more efficient approach, stemming could be a reasonable alternative.\nAn important thing to consider is that we look into words as separate units (tokens) as we saw in the previous episode. For example, think about the word ‚Äúleaves‚Äù. That could both represent the plural of the noun ‚Äúleaf‚Äù or the verb in third person for the word ‚Äúleave‚Äù. That is a good reminder of always remember to apply part of speech (POS) because lemmatization algorithms utilize a lexicon with linguistic rules based on pre-determined tags to avoid misinterpretation.\nPart of Speech (POS) refers to the grammatical category that a word belongs to, indicating its syntactic function and role within a sentence. For example, the word run can serve as a verb in ‚ÄúI like to run every morning‚Äù or as a noun in ‚ÄúI went for a long run‚Äù. Without POS information, an NLP system might incorrectly treat run as always being a verb, producing inaccurate results. By applying POS tagging, systems can correctly recognize each word‚Äôs role, ensuring more accurate text processing.\nAlright, back to our pipeline, we will now convert words to their dictionary form, remove any remaining noise, and finalize our preprocessing steps.",
    "crumbs": [
      "Text Preprocessing",
      "Lemmatization"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/05_lemmatization.html#rebuilding-sentences-comments",
    "href": "chapters/1.Preprocessing/05_lemmatization.html#rebuilding-sentences-comments",
    "title": "Lemmatization",
    "section": "Rebuilding Sentences (Comments)",
    "text": "Rebuilding Sentences (Comments)\nAfter tokenization, our data consists of individual words. However, in order to preserve the ability to apply lemmatization while taking into account each word‚Äôs part of speech (POS), we need to first reconstruct sentences; otherwise, the lemmatizer would operate on isolated tokens without context, which can lead to incorrect or less accurate base forms.\nTo ensure the words are reassembled in the correct order for each original text, we rely on the ID column. Having an ID column is crucial because it allows us to track which words belong to which original text, preventing confusion or misalignment when reconstructing our comments into sentences, especially in large or complex datasets.\nrejoined &lt;- nonstopwords %&gt;%\n  group_by(id) %&gt;%  # group all tokens from the same sentence\n  summarise(comments = paste(word, collapse = \" \"), .groups = \"drop\")",
    "crumbs": [
      "Text Preprocessing",
      "Lemmatization"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/05_lemmatization.html#applying-lemmatization",
    "href": "chapters/1.Preprocessing/05_lemmatization.html#applying-lemmatization",
    "title": "Lemmatization",
    "section": "Applying Lemmatization",
    "text": "Applying Lemmatization\nNext, we will be using creating a new dataframe named lemmatized using the lemmatize_strings() function from the textstem package, and a new column called comments to it, containing the dictonary form of each word.\n# Applying Lemmas\nlemmatized &lt;- rejoined %&gt;%\n  mutate(comments = lemmatize_strings(comments))\nGreat! Let‚Äôs take a look at the lemmatized data frame. For example, words such as ‚Äútelling‚Äù and ‚Äúcaptivating‚Äù were converted into ‚Äútell‚Äù and ‚Äúcaptivate‚Äù.\n\nWait a second! If we look closely, we‚Äôll notice an outlier lemma. Do you see the number two in the third row? This is a known issue with the textstem package. While it hasn‚Äôt been fully resolved yet, we can apply a workaround to address it:\ncustom_dict &lt;- as.data.frame(lexicon::hash_lemmas)\n\n# Find rows where token is \"second\"\nidx &lt;- custom_dict$token == \"second\"\n\n# Set lemma = token to preserve the word\ncustom_dict$lemma[idx] &lt;- custom_dict$token[idx]\n\n# Now lemmatize your text\nlemmatized_nonumbers &lt;- rejoined\ncustom_dict &lt;- as.data.frame(lexicon::hash_lemmas, stringsAsFactors = FALSE)\nLet‚Äôs look at the third row once again in the new dataframe we have created.\nAlright! Problem solved. Keep in mind, however, this would apply to most words referring to numbers, but to save us time let‚Äôs address only this specifc case.",
    "crumbs": [
      "Text Preprocessing",
      "Lemmatization"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/05_lemmatization.html#saving-your-work-for-analysis",
    "href": "chapters/1.Preprocessing/05_lemmatization.html#saving-your-work-for-analysis",
    "title": "Lemmatization",
    "section": "Saving your Work for Analysis",
    "text": "Saving your Work for Analysis\nLet‚Äôs save our work as a new file named comments_preprocessed:\n# Save to CSV\nwrite.csv(lemmatized_nonumbers, \"./data/preprocessed/comments_preprocessed.csv\")",
    "crumbs": [
      "Text Preprocessing",
      "Lemmatization"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/05_lemmatization.html#before-we-go",
    "href": "chapters/1.Preprocessing/05_lemmatization.html#before-we-go",
    "title": "Lemmatization",
    "section": "Before we go",
    "text": "Before we go\nIf you notice that the output object saved as comments_preprocessed contains one fewer row (observation) than the original dataset, this is because, after applying normalization and stopword removal with the Snowball Lexicon, the comment associated with ID \"s2_0998\" became an empty string.\n\nWell done! That concludes all our preprocessing steps. Let‚Äôs now cover some important considerations for your future text preprocessing projects.",
    "crumbs": [
      "Text Preprocessing",
      "Lemmatization"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/03_tokenization.html",
    "href": "chapters/1.Preprocessing/03_tokenization.html",
    "title": "Word Tokenization",
    "section": "",
    "text": "Tokenization in NLP differs from applications in security and blockchain. It corresponds to the action of breaking down text into smaller pieces (aka tokens). It is a foundational process in the digital world, allowing machines to interpret and analyze large volumes of text data. By dividing text into smaller, more manageable units, it enhances both the efficiency and accuracy of data processing.\nText can be tokenized into sentences, word, subwords or even characters, depending on project goals and analysis plan. Here is a summary of these approaches:\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\nCommon Use Cases\n\n\n\n\nSentence Tokenization\nSplits text into individual sentences\n\"I love NLP. It's fascinating!\" ‚Üí [\"I love NLP.\", \"It's fascinating!\"]\nIdeal for tasks like summarization, machine translation, and sentiment analysis at the sentence level\n\n\nWord Tokenization\nDivides text into individual words\n\"I love NLP\" ‚Üí [\"I\", \"love\", \"NLP\"]\nWorks well for languages with clear word boundaries, such as English\n\n\nCharacter Tokenization\nBreaks text down into individual characters\n\"NLP\" ‚Üí [\"N\", \"L\", \"P\"]\nUseful for languages without explicit word boundaries or for very fine-grained text analysis\n\n\nSubword Tokenization\nDecomposes words into smaller parts, like prefixes, suffixes, or common morphemes (the smallest units of meaning in a language)\n\"subword tokenization\" ‚Üí [\"sub\", \"word\", \"token\", \"ization\"]\nEffective for handling rare or unknown words and languages with complex word formation\n\n\n\nSome might recall, that along with the popularization and excitement around ChaGPT, there were also a few warnings about the LLMs failing in answering correctly how many ‚Äúr‚Äù letters does the word strawberry have. Can you guess why?\n\n\n\nImage from: OpenAI Community Forum [829618]\n\n\nAlthough this issue has been resolved in later versions of the model, it was originally caused by subword tokenization. In this case, the tokenizer would split ‚Äústrawberry‚Äù into ‚Äúst,‚Äù ‚Äúraw,‚Äù and ‚Äúberry.‚Äù As a result, the model would incorrectly count the letter ‚Äúr‚Äù only within the ‚Äúberry‚Äù token. This illustrates how the tokenization approach directly affects how words are segmented and how their components are interpreted by the model.\nWhile this is beyond the scope of the workshop, it‚Äôs important to note that some advanced AI models use neural networks to dynamically determine token segmentation. Rather than relying on fixed rules, these models can adapt based on the contextual cues within the text. However, tokenization remains inherently limited by the irregular, organic, and often unpredictable nature of human language.\nLet‚Äôs go back to our dataset and apply tokenization. We will use the `tidytext` function unnest_tokens(), which splits the text in a specified column into individual tokens (here, words), creating one row per token while keeping other data intact. The first argument (word) names the new column for tokens, and the second (text_cleaned) specifies the text column to tokenize. In short, it turns each word from text_cleaned into its own row in a new column called word in a new dataframe named tokenized.\n# Tokenization\ntokenized &lt;- comments %&gt;%\n  unnest_tokens(word, text_cleaned)\nGreat! We‚Äôve just tokenized each individual comment (text string) in our dataset into individual words, giving us, as noted in the environment a total of 121,360 tokens in the tokenized dataframe!\n\nIf you‚Äôve looked at the token output, you may have noticed that some of these tokens are common but less meaningful words, like to, it, and is. Don‚Äôt worry; we‚Äôll take care of those in the next step: stop word removal.",
    "crumbs": [
      "Text Preprocessing",
      "Word Tokenization"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/01_introduction.html",
    "href": "chapters/1.Preprocessing/01_introduction.html",
    "title": "Introduction to Text Preprocessing",
    "section": "",
    "text": "Text analysis is about digging into written words to find meaning, spotting patterns, tracking sentiment, or identifying key topics and themes. It‚Äôs essentially the ‚Äúwhat does this text tell us?‚Äù part of the process. Text analysis turns unstructured words into structured insights. Every day, countless emails, news articles, social media content, scientific literature, and reports are produced, hiding patterns, opinions, and signals that numbers alone can‚Äôt capture. By applying computational text analysis, we can uncover trends, measure public sentiment, compare perspectives, and extract meaning from massive collections of information that would otherwise feel overwhelming.\nText preprocessing is the set of steps used to clean, standardize, and structure raw text before it can be meaningfully analyzed. This may include removing punctuation, normalizing letter case, eliminating stop words, breaking text into tokens (words or sentences), and reducing words to their base forms through lemmatization. Preprocessing reduces noise and inconsistencies in the text, making it ready for computational analysis.\nNatural Language Processing (NLP) provides the computational methods that make text analysis possible. It is a branch of artificial intelligence that enables computers to understand, interpret, and generate human language. NLP techniques include part-of-speech tagging, named entity recognition, sentiment detection, topic modeling, and transforming words into numerical representations for analysis.\nThese three elements are co-dependent and work together. Preprocessing prepares messy text for analysis, NLP provides the computational tools to process and interpret the text, and text analysis applies these insights to answer meaningful questions. Without preprocessing, NLP models struggle with noise; without NLP, text analysis would be limited to surface-level counts; and without text analysis, NLP would have little purpose beyond technical processing. Together, they transform raw text into structured, actionable insights.From messy to analysis-ready text\nOf course, text isn‚Äôt quite as ‚Äúanalysis-ready‚Äù as numbers. Have you ever looked at raw text data and thought, where do I even start? That‚Äôs the challenge: before computers can process it meaningfully, text usually needs some cleaning and preparation. Before text can be analyzed computationally, it needs to be standardized. Computers see ‚ÄúHappy,‚Äù ‚Äúhappy,‚Äù and ‚ÄúHAPPY!!!‚Äù as different words ‚Äî preprocessing fixes that.\nIt‚Äôs extra work, but it‚Äôs also the foundation of any meaningful analysis. The exciting part is what happens next; once the text is shaped and structured, it can reveal insights you‚Äôd never notice just by skimming. And here‚Äôs the real advantage: computers can process enormous amounts of text not only faster but often more effectively than humans, allowing us to see patterns and connections that would otherwise stay hidden.",
    "crumbs": [
      "Text Preprocessing",
      "Introduction to Text Preprocessing"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/01_introduction.html#getting-things-started",
    "href": "chapters/1.Preprocessing/01_introduction.html#getting-things-started",
    "title": "Introduction to Text Preprocessing",
    "section": "Getting Things Started",
    "text": "Getting Things Started\nThe data we pulled for this exercise comes from real social media posts, meaning they are inherently messy, and we know that even before going in. Because it is derived from natural language, this kind of data is unstructured, often filled with inconsistencies and irregularities.\nBefore we can apply any meaningful analysis or modeling, it‚Äôs crucial to visually inspect the data to get a sense of what we‚Äôre working with. Eyeballing the raw text helps us identify common patterns, potential noise, and areas that will require careful preprocessing to ensure the downstream tasks are effective and reliable.\n\nGetting Files and Launching RStudio\nTime to launch RStudio and our example! Click on this link to download the text-preprocessing subfolder, from the folder text-analysis-series. Among other files, this subfolder contains the dataset we will be using comments.csv, a worksheet in qmd, a Quarto extension (learn more about Quarto), named preprocessing-workbook.qmd where we will be performing some coding, and an renv.lock(learn more about Renv) file listing all the R packages (and their versions) we‚Äôll use during the workshop.\nThis setup ensures a self-contained environment, so you can run everything needed for the session without installing or changing any packages that might affect your other R projects.\nAfter downloading this subfolder, double click on the project file text-preprocessing.Rproj to launch Rstudio. Look for and open the file preprocessing-workbook.qmd on your Rstudio environment.\n\n\nSetting up the environment with renv\nNext, we will need to install the package `renv` so you can setup the working environment correctly with all the packages and dependencies we will need. On the console, type:\ninstall.packages(\"renv\")\nThen, still in the console, we will restore it, which will essentially installs packages in an R project to match the versions recorded in the project‚Äôs renv.lock file we have shared with you.\nrenv::restore()\n\n\n\n\n\n\nWarning\n\n\n\nMatrix Package Incompatible with R\nIf you encounter incompatibility issues with the Matrix package (or any other) due to your R version, you can explicitly install the package by running the following in your console:\nrenv::install(\"Matrix\")\nNext, update your renv.lock file to reflect this version by running:\nrenv::snapshot()\n\n\n\n\nLoading Packages & Inspecting the Data\nLet‚Äôs start by loading all the required packages that are pre-installed in the project:\nlibrary(tidyverse)    # general data manipulation\nlibrary(tidytext)     # tokenization and text processing\nlibrary(stringr)      # string manipulation\nlibrary(stringi)      # emoji handling\nlibrary(dplyr)        # data wrangling\nlibrary(textclean)    # expand contractions\nlibrary(emo)          # emoji dictionary\nlibrary(textstem)     # lemmatization\nAfter running it, you should get:\n\nAlright! With all the necessary packages loaded, let‚Äôs take a look at the dataset we‚Äôll be working with:\n# Inspecting the data\ncomments &lt;- readr::read_csv(\"./data/raw/comments.csv\")\nWhich should show our dataset contains 5877 comments and two columns and display the comments dataset to our environment:\n\nIn the workbook, you‚Äôll notice that we‚Äôve pre-populated some chunks below to save you from the tedious typing. Don‚Äôt worry about them for now, we‚Äôll come back to them shortly.\n\n\n\n\n\n\nNoteüí¨ Discussion\n\n\n\nWorking in pairs or trios, look briefly by double clicking comments dataset in the environment panel. Then, discuss could be potential challenges of analyzing this text on its current form. What could be potential areas of friction that could compromise the results?",
    "crumbs": [
      "Text Preprocessing",
      "Introduction to Text Preprocessing"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu ) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu",
    "crumbs": [
      "About RDS"
    ]
  },
  {
    "objectID": "about.html#ways-we-can-help-you",
    "href": "about.html#ways-we-can-help-you",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu ) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu",
    "crumbs": [
      "About RDS"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html",
    "href": "chapters/1.Preprocessing/02_normalization.html",
    "title": "Normalization & Noise Reduction",
    "section": "",
    "text": "As previously mentioned, in order to perform accurate and reliable analysis, we need to ‚Äútake out the garbage‚Äù first by preprocessing the text to clean, standardize, and structure the input data. These steps help reduce noise and improve the model‚Äôs accuracy.\nBelow we use another analogy to illustrate the impact of noise on data analysis outcomes. Imagine a tree that is slowly dying. On the surface, its leaves may still appear green, but closer inspection reveals branches that are brittle, bark that is cracking, and roots that are struggling to find nourishment. If we only focus on the healthy-looking leaves, we might draw a misleading conclusion about the tree‚Äôs overall condition. Similarly, in text analysis, raw data often contains ‚Äúnoise,‚Äù such as irrelevant words, inconsistent formatting, or errors, which can obscure meaningful patterns. If we feed this noisy data directly into an analysis, the results can be skewed, incomplete, or misleading, just as judging the tree‚Äôs health by its leaves alone would be.\nJust as a gardener would prune dead branches, enrich the soil, and care for the roots to revive the tree, data analysts perform preprocessing steps to clean, standardize, and structure the text. By removing noise and focusing on the core content, we give the analysis the best chance to reveal true insights, uncover trends, and support reliable conclusions. In short, the quality of our ‚Äúdata garden‚Äù directly determines the health of the insights it produces.\n\nThe main goal of normalization is to remove irrelevant content and standardize the data in order to reduce noise. Below are some key actions we‚Äôll be performing during this workshop:\n\n\n\n\n\n\n\nAction\nWhy it matters?\n\n\n\n\nRemove URLs\nURLs often contain irrelevant noise and don‚Äôt contribute meaningful content for analysis.\n\n\nRemove Punctuation & Symbols\nPunctuation marks and other symbols including those extensively used in social media for mentioning (@) or tagging (#) rarely adds value in most NLP tasks and can interfere with tokenization (as we will cover in a bit) or word matching.\n\n\nRemove Numbers\nNumbers can be noise in most contexts unless specifically relevant (e.g., in financial or medical texts) don‚Äôt contribute much to the analysis. However, in NLP tasks they are considered important, there might be considerations to replace them with dummy tokens (e.g.¬†&lt;NUMBER&gt;), or even converting them into their written form (e.g, 100 becomes one hundred).\n\n\nNormalize Whitespaces\nEnsures consistent word boundaries and avoids issues during tokenization or frequency analysis.\n\n\nConvert to Lowercase\nPrevents case sensitivity from splitting word counts due to case variations (e.g., ‚ÄúAppleTV‚Äù ‚â† ‚ÄúAPPLETV‚Äù ‚â† ‚ÄúappleTV‚Äù ‚â† ‚Äúappletv‚Äù), improving model consistency.\n\n\nConvert Emojis to Text\nEmojis play a unique role in text analysis, as they often convey sentiment. Rather than removing them, we will convert them into their corresponding text descriptions.\n\n\n\n\n\n\n\n\n\nNoteüß† Knowledge Check\n\n\n\nIn pairs or groups of three, identify the techniques you would consider using to normalize and reduce noise in the following sentence:\n‚ÄúOMG!! üò± I can‚Äôt believe it‚Ä¶ This is CRAZY!!! #unreal ü§Ø‚Äù\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nHow many techniques could you identify? Bingo if you have spotted all four!\nAfter applying them the sentence should look like:\nomg [face scream in fear] I can not believe it this is crazy unreal [exploding head]\n\n\n\n\n\nA caveat when working with emojis is that they are figurative and highly contextual. Also, there may be important generational and cultural variability in how people interpret them. For example, some countries may use the Folded Hands Emoji (üôè) as a sign of thank you where others may seem as religious expression. Also, some may use it in a more positive way as gratitude, hope or respect, or in a negative context, where they might be demonstrating submission or begging.\nYou might have noticed based on the example above that emojis are converted to their equivalent CLDR (common, human-readable name) based on this emoji unicode list, which are not as nuanced and always helpful to detect sentiment. While not always perfect, that is an important step to normalize the data and we will see how this process looks like later on this episode.",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#the-role-of-regular-expressions",
    "href": "chapters/1.Preprocessing/02_normalization.html#the-role-of-regular-expressions",
    "title": "Normalization & Noise Reduction",
    "section": "The Role of Regular Expressions",
    "text": "The Role of Regular Expressions\nRegular expressions (regex) are powerful tools for pattern matching and text manipulation. They allow you to identify, extract, or replace specific sequences of characters in text, such as email addresses, URLs, hashtags, or user mentions. In text cleaning, regex is essential for reducing noise by removing unwanted elements like punctuation, special symbols, or repeated whitespace, which can interfere with analysis. By systematically filtering out irrelevant or inconsistent text, regex helps create cleaner, more structured data, improving the accuracy of downstream tasks like sentiment analysis, topic modeling, or machine learning.\nEven though we won‚Äôt dive into the syntax or practice exercises here, being aware of regex and its capabilities can help you understand how text preprocessing works behind the scenes and guide you toward resources to learn it on your own.\nWorking with regular expressions might require some trial and error, especially when you are working with a large and highly messy corpus. To make things easier and make the lesson less typing-intensive, we‚Äôve pre-populated the workbook with regex patterns noted below and will provide a clear explanation of how they are expected to function, so you can follow along confidently.\nurl_pattern &lt;- \"http[s]?://[^\\\\s,]+|www\\\\.[^\\\\s,]+\"\nhidden_characters &lt;- \"[\\u00A0\\u2066\\u2067\\u2068\\u2069]\"\napostrophes &lt;- (\"[‚Äò‚Äô‚Äõ º‚ùõ‚ùúÔºá`¬¥‚Ä≤]\")\nmentions &lt;- \"@[A-Za-z0-9_]+\"\nhashtag_splitter &lt;- \"(?&lt;![#@])([a-z])([A-Z])\"\npunctuation &lt;- \"[[:punct:]‚Äú‚Äù‚Äò‚Äô‚Äì‚Äî‚Ä¶|+]\"\nnumbers &lt;- \"[[:digit:]]\"\nrepeated_chars &lt;- \"(.)\\\\1{2,}\"\nsingle_characters &lt;- \"\\\\b[a-zA-Z]\\\\b\"\nLet‚Äôs run this chunk with pattern variables for now, we will get back to each line when covering their corresponding code in our normalization and noise reduction chunk. Notice this patterns will show in our environment as seen below:\n\n\n\n\n\n\n\nTipGet Help with Regex\n\n\n\nTesting regular expressions is essential for accuracy and reliability, since complex patterns often produce unexpected results. Careful testing ensures your regex matches the intended text, rejects invalid inputs, and performs efficiently, while also revealing potential bugs before they impact your system. To make testing more effective, use tools like Regex101 or the Coder Pad cheatsheet or and be sure to check tricky border cases that might otherwise slip through.\n\n\n\n\n\n\n\n\nCautionThe Order Matters!\n\n\n\nBefore we start performing text normalization and noise reduction, we should caution you that the order of steps matters because each transformation changes the text in a way that can affect subsequent steps. Doing things in a different order can lead to different results, and sometimes even incorrect or unexpected outcomes. For example, if we remove punctuation before expanding contractions, \"can't\" might turn into \"cant\" instead of \"cannot\", losing the correct meaning.\n\n\nAlright, let‚Äôs return to our workbook to dive into cleaning and normalization. The order in which we apply these steps matters, each transformation builds on the previous one to make the data more consistent, structured, and analysis-ready. Keep in mind, however, that the pipeline we‚Äôll use here is unlikely to perfectly fit every type of textual data; the best approach always depends on your specific dataset and project goals.",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#creating-a-new-data-frame",
    "href": "chapters/1.Preprocessing/02_normalization.html#creating-a-new-data-frame",
    "title": "Normalization & Noise Reduction",
    "section": "0. Creating a New Data Frame",
    "text": "0. Creating a New Data Frame\nThe first step would be to create a new data frame called comments_clean and adding a clean_text column to it:\n# Create a new column for the output clean text\ncomments_clean &lt;- comments %&gt;%\n  mutate(\n    clean_text = text %&gt;%\nDon‚Äôt forget the pipe operator `%` since we want to pass the result of this function as an input to the next, as we continue working on the code chunk.",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#removing-urls",
    "href": "chapters/1.Preprocessing/02_normalization.html#removing-urls",
    "title": "Normalization & Noise Reduction",
    "section": "1. Removing URLs",
    "text": "1. Removing URLs\nThis comes first because URLs can contain other characters (like punctuation or digits) that you‚Äôll handle later, so removing them early prevents interference. Because we have a great variation in format (e.g., http://, https://, or www.) we had to define a regex pattern, where:\n\nhttp[s]?:// ‚Üí matches ‚Äúhttp://‚Äù or ‚Äúhttps://‚Äù\n[^\\\\s,]+ ‚Üí matches one or more characters that are not spaces or commas (the rest of the URL)\n| ‚Üí OR operator; matches either the left or right pattern\nwww\\.[^\\\\s,]+ ‚Üí matches URLs starting with ‚Äúwww.‚Äù followed by non-space/non-comma characters\n\nSince we already have the URL pattern, we can use str_replace_all() from the stringr package to replace all matching URLs with an empty string in our text.\n# Remove URLs\nstr_replace_all(url_pattern, \"\") %&gt;%",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#hidden-characters",
    "href": "chapters/1.Preprocessing/02_normalization.html#hidden-characters",
    "title": "Normalization & Noise Reduction",
    "section": "2. Hidden Characters",
    "text": "2. Hidden Characters\nHidden characters are one of the trickiest challenges in text preprocessing. These are characters that aren‚Äôt immediately visible when you view text, but they can interfere with analysis, parsing, or modeling. Hidden characters include things like non-breaking spaces, zero-width spaces, invisible control characters (like carriage returns \\r, line feeds \\n, tabs \\t), Unicode invisible formatting characters, and even special symbols copied from PDFs or web pages.\nIn practice, these characters can cause subtle errors. For example, they can make two strings appear different even though they look identical, break tokenization, or create issues when matching patterns with regular expressions. In natural language processing (NLP), they can inflate vocabulary size unnecessarily, confuse word embeddings, or lead to inaccurate frequency counts.\nTo ensure they won‚Äôt cause us future problems, we have included a regex pattern that matches certain hidden or invisible Unicode characters in text. Here‚Äôs a breakdown:\n\n[] ‚Üí This denotes a character class in regex, meaning it will match any single character listed inside.\n\\u00A0 ‚Üí This is a non-breaking space. Unlike a normal space, it doesn‚Äôt allow line breaks. It often appears when copying text from websites or PDFs.\n\\u2066, \\u2067, \\u2068, \\u2069 ‚Üí These are Unicode ‚Äúisolate‚Äù control characters used for bidirectional text handling (like in right-to-left languages). They are invisible and generally unnecessary for text analysis.\n\nLet‚Äôs now call that variable and enter some code to address that:\n# Replace hidden/special characters with space\nstr_replace_all(hidden_characters, \" \") %&gt;%",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#handling-apostrophes",
    "href": "chapters/1.Preprocessing/02_normalization.html#handling-apostrophes",
    "title": "Normalization & Noise Reduction",
    "section": "3. Handling Apostrophes",
    "text": "3. Handling Apostrophes\nThis step helps clean up text by making sure all apostrophes are consistent, rather than a mix of fancy Unicode versions. Applying it to the text column in our comments dataset should look like. In this case, the pattern ‚Äú[‚Äô‚Äò º]‚Äù looks for several different kinds of apostrophes and backticks; like the left and right single quotes, the modifier apostrophe, and the backtick. Each of those gets replaced with a simple, standard apostrophe (‚Äô`) when we apply the function below:\n# Standardize apostrophes\nstr_replace_all(apostrophes, \"'\") %&gt;%\nNote that once again, we are calling the stored variable with the regex containing different forms of apostrophes, especially from social media, PDFs, or copy-pasted content.",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#expanding-contractions",
    "href": "chapters/1.Preprocessing/02_normalization.html#expanding-contractions",
    "title": "Normalization & Noise Reduction",
    "section": "4. Expanding Contractions",
    "text": "4. Expanding Contractions\nNow that we have normalized variations of apostrophes, we can properly handle contractions. In everyday language, we often shorten words: can‚Äôt, don‚Äôt, it‚Äôs. These make speech and writing flow more easily, but they can cause confusion for Natural Language Processing (NLP) models. Expanding contractions, such as changing can‚Äôt to cannot or it‚Äôs to it is, helps bring clarity and consistency to the text because NLP models treat don‚Äôt and do not as completely different words, even though they mean the same thing. Also, words like cant, doesnt, and whats lose their meaning. Expanding contractions reduces this inconsistency and ensures that both forms are recognized as the same concept. Expanding it to is not happy makes the negative sentiment explicit, which is especially important in tasks like sentiment analysis.\nSo, while it may seem like a small step, it often leads to cleaner data, leaner models, and more accurate results. First, however, we need to ensure that apostrophes are handled correctly. It‚Äôs not uncommon to encounter messy text where nonstandard characters are used in place of the straight apostrophe (‚Äô). Such inconsistencies are very common and can disrupt contraction expansion.\n\n\n\n\n\n\n\n\nCharacter\nUnicode\nNotes\n\n\n\n\n'\nU+0027\nStandard straight apostrophe, used in most dictionaries\n\n\n‚Äô\nU+2019\nRight single quotation mark (curly apostrophe)\n\n\n‚Äò\nU+2018\nLeft single quotation mark\n\n\n º\nU+02BC\nModifier letter apostrophe\n\n\n`\nU+0060\nGrave accent (sometimes typed by mistake)\n\n\n\nTo perform this step we will be using the function replace_contraction from the textclean package to make sure that words like ‚Äúdon‚Äôt‚Äù become ‚Äúdo not‚Äù, by adding the following line to our code chunk:\n# Expand contractions (e.g., \"don't\" ‚Üí \"do not\")\nreplace_contraction() %&gt;%",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#removing-mentions",
    "href": "chapters/1.Preprocessing/02_normalization.html#removing-mentions",
    "title": "Normalization & Noise Reduction",
    "section": "5. Removing Mentions",
    "text": "5. Removing Mentions\nContinuing with our workflow, we will now handle direct mentions and usernames in our dataset, as they do not contribute relevant information to our analysis. We will use a function to replace all occurrences of usernames preceded. Since we have pre-populated the regular expression and stored it in the variable ‚Äúmentions‚Äù, mentions &lt;- \"@[A-Za-z0-9_]+\" we will only need to add that we want to replace it with an empty string and remove them.\n# Remove mentions (@username)\nstr_replace_all(mentions, \"\") %&gt;%",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#splitting-hashtags",
    "href": "chapters/1.Preprocessing/02_normalization.html#splitting-hashtags",
    "title": "Normalization & Noise Reduction",
    "section": "6. Splitting Hashtags",
    "text": "6. Splitting Hashtags\nHashtags are a powerful tool in social media and online communication, serving as a way to categorize content, highlight topics, and increase visibility. By prefixing a word or phrase with #, users can link their posts to a broader conversation, making it easier for others to discover and engage with content on that topic. Hashtags also act as a keyword system, allowing researchers and analysts to track trends, measure sentiment, or analyze public discussions around specific themes. In text analysis, properly handling hashtags ensures that the meaningful content within them is captured, improving tokenization, searchability, and overall insights from the data. They effectively turn user-generated text into a structured source of topical information that can be leveraged for both social and analytical purposes.\nHashtags are often written as one long string with no spaces, e.g., #SeveranceIsFire. Splitting them into separate words not only makes it human-readable and easier to understand, but also ensure meaningful tokes and improves text analysis accuracy.\nWhile some researchers might want to separate them from the text for further analysis, we can also split concatenated words in hashtags or camelCase text by inserting a space between them. In our regex pattern hashtag_splitter, we have:\n\n(?&lt;![#@]): negative lookbehind to avoid splitting right after # or @, preserving hashtags and mentions.\n([a-z])([A-Z]): captures lowercase followed by uppercase letters, identifying camelCase word boundaries.\n\nWe can call that pattern and use the string replacement function to adds the space between words while keeping the letters themselves intact, by adding the following to our code chunk:\n# Split camelCase hashtags (optional)\nstr_replace_all(hashtag_splitter, \"\\\\1 \\\\2\") %&gt;%\nIn simple terms, the \"\\1 \\2\" inserts the first captured piece, adds a space, and then inserts the second captured piece.\nYou might be asking, ‚Äúwait a minute but what about the hastag (#) itself?‚Äù. It is a valid question, but not to worry, we will take care of that later when handling symbols.",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#converting-to-lowercase",
    "href": "chapters/1.Preprocessing/02_normalization.html#converting-to-lowercase",
    "title": "Normalization & Noise Reduction",
    "section": "7. Converting to Lowercase",
    "text": "7. Converting to Lowercase\nHaving all text converted to lowercase will be our next step, by adding the following line to our code chunk:\n# Convert to lowercase\nstr_to_lower() %&gt;%",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#cleaning-punctuation-symbols-and-numbers",
    "href": "chapters/1.Preprocessing/02_normalization.html#cleaning-punctuation-symbols-and-numbers",
    "title": "Normalization & Noise Reduction",
    "section": "8. Cleaning Punctuation, Symbols and Numbers",
    "text": "8. Cleaning Punctuation, Symbols and Numbers\nAlright, time to remove punctuation and symbols, and then numbers.\nBut first, last break it down, the punctuation regex pattern, where [[:punct:]] is character class in regex that matches any standard punctuation character, including: ! ‚Äù # $ % & ‚Äô ( ) * + , - . / : ; &lt; = &gt; ? @ [ ¬†] ^ _ ` { | } ~. To be safe and because our dataset is really messy, we added extra characters (‚Äú‚Äù‚Äò‚Äô‚Äì‚Äî‚Ä¶|+) to catch some special quotes, dashes, ellipsis, and symbols that [[:punct:]] might miss. Calling that variable, we can remove them by adding to our code:\n# Remove punctuation\nstr_replace_all(punctuation, \" \") %&gt;%\nNext, last clear some numbers by adding [[:digit:]]+. This is the regex pattern matches any single digit (0‚Äì9) and + means one or more digits in a row. So it matches sequences like 7, 42, 2025, etc:\n# Remove digits\nstr_replace_all(\"[[:digit:]]+\", \" \") %&gt;%",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#handling-elongation",
    "href": "chapters/1.Preprocessing/02_normalization.html#handling-elongation",
    "title": "Normalization & Noise Reduction",
    "section": "9. Handling Elongation",
    "text": "9. Handling Elongation\nIn user-generated content, it‚Äôs common to see repeated letters used for emphasis (e.g., ‚ÄúAmaaaazing,‚Äù ‚ÄúLoooove‚Äù). For that, we have the regex pattern repeated_chars &lt;- \"(.)\\1{2,}\", where:\n\n(.): captures group that matches any single character (except line breaks, depending on regex flavor). The . is a wildcard. The parentheses () capture whatever character matched for later reference.\n\\\\1: refers to ‚Äúwhatever was matched by the first capturing group.‚Äù In other words, it matches the same character again.\n{2,}: means ‚Äúrepeat the previous element at least 2 times.‚Äù\n\nPutting it together (.)\\\\1{2,} matches any character that repeats 3 or more times consecutively. Why 3? Because the first occurrence is matched by (.) and {2,} requires at least 2 more repetitions, so total = 3+.\nBecause the pattern is already in our workbook, we can simply add:\n# Normalize repeated characters (e.g., loooove ‚Üí love)\nstr_replace_all(repeated_chars, \"\\\\1\") %&gt;%\nCan you guess what \"\\1\" does? It uses the character we captured earlier, keeping it in the text.",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#convert-emojis-to-text",
    "href": "chapters/1.Preprocessing/02_normalization.html#convert-emojis-to-text",
    "title": "Normalization & Noise Reduction",
    "section": "10. Convert Emojis to Text",
    "text": "10. Convert Emojis to Text\nOkay, now we‚Äôll convert emojis into their text descriptions to make them machine-readable, using the emoji package to help with this step. We need to first load the emoji dictionary:\n\nGetting the emoji dictionary\n# Load the emoji dictionary\nemoji_dict &lt;- emo::jis[, c(\"emoji\", \"name\")]\nemoji_dict\nTake a look at the emoji dictionary we loaded into our RStudio environment. It‚Äôs packed with more emojis and some surprising meanings than you might expect.\nWe will then write a separate function to deal with those emojis in our dataset:\n# Function to replace emojis in text with their corresponding names\nreplace_emojis &lt;- function(text, emoji_dict) {\n  stri_replace_all_fixed(\n    str = text,                  # The text to process\n    pattern = emoji_dict$emoji,  # The emojis to find\n    replacement = paste0(emoji_dict$name, \" \"), # Their corresponding names\n    vectorize_all = FALSE        # element-wise replacement in a same string\n  )\n}\nWait, we are not done yet! We still have to add the replace_emojis function, based on our loaded dictionary, into our code chunk. This will replace the emojis with their corresponding text on our dataset:\n# Replace emojis with textual description (function - see above)\nreplace_emojis(emoji_dict) %&gt;%\nNeat! Let‚Äôs re-run the code chunk and check it once again. With emojis taken care of, we can now apply the next and final normalization and noise reduction step.",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#removing-single-characters",
    "href": "chapters/1.Preprocessing/02_normalization.html#removing-single-characters",
    "title": "Normalization & Noise Reduction",
    "section": "13. Removing single characters",
    "text": "13. Removing single characters\nAfter removing digits and handling contractions, some single characters may remain in the text. For example:\n\n\"S1\" (referring to ‚ÄúSeason one‚Äù) would become \"s\" after digit removal.\nPossessive forms like \"Mark's\" would turn into \"Mark s\" once the apostrophe is removed.\n\nThese isolated single characters generally do not carry meaningful information for analysis, so we should remove them from the dataset to keep our text clean and focused.\nWe will be using the function str_replace_all() comes from the stringr package:\n# Remove single characters (e.g S1 which became S)\nstr_replace_all(single_characters, \"\") %&gt;%",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#dealing-with-extraspaces",
    "href": "chapters/1.Preprocessing/02_normalization.html#dealing-with-extraspaces",
    "title": "Normalization & Noise Reduction",
    "section": "12. Dealing with Extraspaces",
    "text": "12. Dealing with Extraspaces\nAfter completing several normalization steps, we should also account for any extra spaces at the beginning or end of the text. These often come from inconsistent user input, copy-pasting, or formatting issues on different platforms. Let‚Äôs clean up these spaces before moving on to the next episode.\n# Remove extra whitespaces\nstr_squish()\nPutting every step together we should have the following code:\n#Create a new column for the output clean text\ncomments &lt;- comments %&gt;%\n  mutate(\n    text_cleaned = text %&gt;%\n      # Remove URLs\n      str_replace_all(url_pattern, \"\") %&gt;%\n      # Replace hidden/special characters with space\n      str_replace_all(hidden_characters, \" \") %&gt;%\n      # Standardize apostrophes\n      str_replace_all(apostrophes, \"'\") %&gt;%\n      # Expand contractions (e.g., \"don't\" ‚Üí \"do not\")\n      replace_contraction() %&gt;%\n      # Remove mentions (@username)\n      str_replace_all(mentions, \"\") %&gt;%\n      # Split camelCase hashtags\n      str_replace_all(hashtag_splitter, \"\\\\1 \\\\2\") %&gt;%\n      # Convert to lowercase\n      str_to_lower() %&gt;%\n      # Remove punctuation\n      str_replace_all(punctuation, \" \") %&gt;%\n      # Remove digits\n      str_replace_all(\"[[:digit:]]+\", \" \") %&gt;%\n      # Normalize repeated characters (e.g., loooove ‚Üí love)\n      str_replace_all(repeated_chars, \"\\\\1\") %&gt;%\n      # Replace emojis with textual description (function - see above)\n      replace_emojis(emoji_dict) %&gt;%\n      # Remove single characters (e.g S1 which became S)\n      str_replace_all(single_characters, \"\") %&gt;%\n      # Remove extra whitespaces\n      str_squish()\n  )\nNeat! Our normalization and noise reduction code chunk is complete, but don‚Äôt forget to close the parentheses before running it! Let‚Äôs see and compare the original ‚Äútext column‚Äù compares to ‚Äútext_cleaned‚Äù.",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#one-more-thing-before-we-are-done",
    "href": "chapters/1.Preprocessing/02_normalization.html#one-more-thing-before-we-are-done",
    "title": "Normalization & Noise Reduction",
    "section": "One more thing before we are done",
    "text": "One more thing before we are done\nOh, no, if we search for ‚Äús1_1755‚Äù we will notice that we still have a waffle üßá emoji in that comment. Well, not too worry, while this emoji wasn‚Äôt included in the original dictionary we used, we can still add it to it for automatic handling:\n{emoji_dict &lt;- emo::jis[, c(\"emoji\", \"name\")]}\nemoji_dict &lt;- emoji_dict %&gt;% add_row(\"emoji\" = \"üßá\", \"name\" = \"waffle\")\nemoji_dict\n\n\n\n\n\n\nTipü¶æ Challenge\n\n\n\n\n\nIdentify at least one remaining emoji that wasn‚Äôt converted and add it to the emoji dictionary. It is okay if we are focusing on different ones, since the process is the same.\n\nSolution\n# Adding new emojis manually to dictionary \nemoji_dict &lt;- emoji_dict %&gt;% add_row(\"emoji\" = \"üßá\", \"name\" = \"waffle\") %&gt;%\nadd_row(emoji = \"ü•π\", name = \"pleading face\")\nemoji_dict\n\n\n\n\nLet‚Äôs re-run the code chunk and check how those emojis were taken care of. With normalization and noise reduction completed, we are now ready to move to the next preprocessing step: tokenization.\n\n\n\n\n\n\nNoteüìë Suggested Readings\n\n\n\nBai, Q., Dan, Q., Mu, Z., & Yang, M. (2019). A systematic review of emoji: Current research and future perspectives. Frontiers in psychology, 10, https://doi.org/10.3389/fpsyg.2019.02221\nGraham, P. V. (2024). Emojis: An Approach to Interpretation. UC L. SF Commc‚Äôn and Ent. J., 46, 123. https://repository.uclawsf.edu/cgi/viewcontent.cgi?article=1850&context=hastings_comm_ent_law_journal",
    "crumbs": [
      "Text Preprocessing",
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/04_stopwords.html",
    "href": "chapters/1.Preprocessing/04_stopwords.html",
    "title": "Stop Words Removal",
    "section": "",
    "text": "Stop words are commonly occurring words that are usually filtered out during natural language processing, as they carry minimal semantic weight and are not as useful for feature extraction.\nExamples include articles (i.e., a, an, the), prepositions (e.g., in, on, at), conjunctions (and, but, or), and pronouns (they, she, he), but the list goes on. While they appear often in text, they usually don‚Äôt add significant meaning to a sentence or search query.\nBy ignoring stop words, search engines, databases, chatbots and virtual assistants can improve the speed of crawling and indexing and help deliver faster, more efficient results. Similar posistive effects applies to other NLP tasks and models performance, including sentiment analysis.\nFor this workshop, we will be using the package stopwords (more info) which is considered a ‚Äúon-stop stopping‚Äù for R users. For English language, the package relies on the Snowball list. But, before we turn to our worksheet to see how that process looks like and how it will apply to our data, let‚Äôs have a little challenge!\n\n\n\n\n\n\nNoteüß† Knowledge Check\n\n\n\nHow many stop words can you spot in each of the following sentences:\n\nThe cat was sitting on the mat near the window.\nShe is going to the store because she needs some milk.\nI will be there in the morning if it doesn‚Äôt rain.\nThey have been working on the project for several days.\nAlthough he was tired, he continued to walk until he reached the house.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n1. The cat was sitting on the mat near the window.\n2. She is going to the store because she needs some milk.\n3. I will be there in the morning, if it does not rain.\n4. They have been working on the project for several days.\n5. Although he was very tired, he continued to walk until he reached the house.\n\n\n\n\n\nNow, let‚Äôs return to the worksheet and see how we can put that into practice.\nSMART, Snowbal and Onix are the three lexicons available to handle stopwords through the the tidytext ecossytem. They serve the same purpose, removing common, low-information words, but they differ in origin, size, and linguistic design. For this workshop, we will adopt the Snowball list because its less restrictive nature, which helps preserve context, especially important for NLP tasks such as topic modeling, sentiment analysis, or classification.\nWe will start our stop word removal by calling data(\"stop_words\") to load a built-in dataset from the tidytext package. This should create a dictionary containing 1,149 words as part of the lexicon‚Äôs library.\n\nThen, we will apply the expression filter(lexicon == \"snowball\") to select the Snowball source (or lexicon). The double equal sign == it is a comparison operator which checks for equality.\nNext, select(word) line keeps only the column called word, dropping other columns like the source name. This gives you a clean list of Snowball stopwords.\nNext, we will remove stopwords from the tokenized text. The anti_join(..., by = \"word\") function keeps only the words that do not match any word in the Snowball stopword list. The result, stored in nonstopwords, is a dataset containing only the meaningful words from your text, with the common stopwords removed.\nThe code chunk, should look like:\n# Load stop words\ndata(\"stop_words\")\n\n# Filter for Snowball stopwords only (less aggressive than SMART)\nsnowball_stopwords &lt;- stop_words %&gt;%\n  filter(lexicon == \"snowball\") %&gt;%\n  select(word)  # keep only the 'word' column\n\n# Remove stopwords from your tokenized data\nnonstopwords &lt;- tokenized %&gt;%\n  anti_join(snowball_stopwords, by = \"word\")\nAwesome! This step should bring our token count down to 74,264 by removing filler and unnecessary words:\n\nWe are now ready to move to lemmatization.\n\n\n\n\n\n\nNoteüìë Suggested Reading\n\n\n\nCheck out this blog post for a summary of the history of stop words, discussion on its applications and some perspectives on developments in the age of AI.\nGaviraj, K. (2025, April 24). The origins of stop words. BytePlus. https://www.byteplus.com/en/topic/400391?title=the-origins-of-stop-words",
    "crumbs": [
      "Text Preprocessing",
      "Stop Words Removal"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/06_conclusion.html",
    "href": "chapters/1.Preprocessing/06_conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "In this workshop, we navigated the challenges of preprocessing of unustrucutred social media data, highlighting how messy, inconsistent, and noisy real-world datasets can be. One key takeaway is the importance of thoroughly assessing the data in the context of your project goals before diving into processing and be mindful that the order of factors do influence the outcome.\nNot all cleaning or transformation steps are universally beneficial and decisions should be guided by what is meaningful for your analysis or model objectives. Emojis, for example, can convey sentiment, irony, or context that may be essential for analysis, so decisions on whether to remove, convert, or retain them should be goal-driven.\nSimilarly, numbers such as dates, prices, or statistics can carry meaningful information, but they can also introduce noise if misinterpreted or inconsistently formatted. Thoughtful handling of these elements ensures that preprocessing enhances the dataset‚Äôs usefulness rather than stripping away valuable signals.\nOverly aggressive text cleaning removes content that is vital to the context, meaning, or nuance of a text and can damage the performance of natural language processing (NLP) models. The specific steps that lead to this problem depend on the end goal of your NLP task.¬†\nWhile preprocessing is considered a key step, if performed incorrectly or poorly planned, it can do more harm than good to the analysis. In short, preprocessing is not merely a mechanical phase in the pipeline but a thoughtful design choice that shapes the quality, interpretability, and trustworthiness of all subsequent tasks.\nBy critically evaluating the data and aligning preprocessing strategies with the end goals, we can ensure that the cleaned dataset not only becomes more manageable but also more valuable for deriving actionable insights. Ultimately, thoughtful data assessment is just as important as the technical preprocessing steps themselves.\n\n\n\n\n\n\nTipü§ì Suggested Readings\n\n\n\nChai CP. Comparison of text preprocessing methods. Natural Language Engineering. 2023;29(3):509-553. https://doi.org/10.1017/S1351324922000213\nSiino, M., Tinnirello, I., & La Cascia, M. (2024). Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on Transformers and traditional classifiers. Information Systems, 121, 102342. https://doi.org/10.1016/j.is.2023.102342",
    "crumbs": [
      "Text Preprocessing",
      "Conclusion"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/ngrams.html",
    "href": "chapters/2.TextAnalysis/ngrams.html",
    "title": "N-grams and Word Sequences",
    "section": "",
    "text": "As you can notice, counting words can be useful to explore common terms in a text corpus, but it does not capture the context in which words are used. To gain deeper insights into the relationships between words, we can analyze sequences of words, known as n-grams. N-grams are contiguous sequences of ‚Äòn‚Äô items (words) from a given text. For example, a bigram is a sequence of two words, while a trigram is a sequence of three words.",
    "crumbs": [
      "Text Analysis",
      "N-grams and Collocations"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/ngrams.html#creating-n-grams",
    "href": "chapters/2.TextAnalysis/ngrams.html#creating-n-grams",
    "title": "N-grams and Word Sequences",
    "section": "Creating N-grams",
    "text": "Creating N-grams\nBecause creating n-grams involves tokenizing text into sequences of words, we can use the unnest_tokens() function from the tidytext package again, but this time specifying the token argument to create n-grams.\n\n# Creating bigrams (2-grams) from the comments\nngrams &lt;- comments %&gt;%\n  unnest_tokens(ngrams, comments, token = \"ngrams\", n = 2) #bigrams \n\nngrams\n\n# A tibble: 67,100 √ó 3\n    ...1 id      ngrams          \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;           \n 1     1 s1_0001 everyone telling\n 2     1 s1_0001 telling watch   \n 3     1 s1_0001 watch severance \n 4     1 s1_0001 severance nobody\n 5     1 s1_0001 nobody tryna    \n 6     1 s1_0001 tryna watch     \n 7     1 s1_0001 watch apple     \n 8     2 s1_0002 can quite       \n 9     2 s1_0002 quite explain   \n10     2 s1_0002 explain show    \n# ‚Ñπ 67,090 more rows\n\n\nThe resulting ngrams data frame contains bigrams extracted from the comments. Each row represents a bigram, which consists of two consecutive words from the original text.\nBy changing the value of n in the unnest_tokens() function, we can create trigrams (3-grams), four-grams, and so on, depending on our analysis needs.\n\n# Creating trigrams (3-grams) from the comments\ntrigrams &lt;- comments %&gt;%\n  unnest_tokens(ngrams, comments, token = \"ngrams\", n = 3) #trigrams\ntrigrams\n\n# A tibble: 61,297 √ó 3\n    ...1 id      ngrams                    \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;                     \n 1     1 s1_0001 everyone telling watch    \n 2     1 s1_0001 telling watch severance   \n 3     1 s1_0001 watch severance nobody    \n 4     1 s1_0001 severance nobody tryna    \n 5     1 s1_0001 nobody tryna watch        \n 6     1 s1_0001 tryna watch apple         \n 7     2 s1_0002 can quite explain         \n 8     2 s1_0002 quite explain show        \n 9     2 s1_0002 explain show severance    \n10     2 s1_0002 show severance captivating\n# ‚Ñπ 61,287 more rows",
    "crumbs": [
      "Text Analysis",
      "N-grams and Collocations"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/ngrams.html#next-word-prediction-using-n-grams",
    "href": "chapters/2.TextAnalysis/ngrams.html#next-word-prediction-using-n-grams",
    "title": "N-grams and Word Sequences",
    "section": "Next Word Prediction Using N-grams",
    "text": "Next Word Prediction Using N-grams\nOne practical application of n-grams is in building simple predictive text models. For instance, we can create a function that predicts the next word based on a given word using bigrams.\n\n# Function to predict the next word based on a given word using bigrams\nnext_word &lt;- function(word, ngrams_df) {\n    matches &lt;- ngrams_df %&gt;%\n        separate(ngrams, into = c(\"w1\", \"w2\"), sep = \" \", remove = FALSE) %&gt;%\n        filter(w1 == word) %&gt;%\n        pull(w2)\n    freq &lt;- table(matches)\n    nw &lt;- max(freq)\n    return(names(freq[freq == nw]))\n}\n\nThis function takes a word and the n-grams data frame as inputs, finds all bigrams where the first word matches the input word, and returns the most frequently occurring second word as the predicted next word.\nWe can see how this function works by providing an example:\n\ntype_any_word = \"ben\"\n\nnext_word(type_any_word, ngrams)\n\n[1] \"stiller\"\n\n\nWe can even play with a simple loop to see how the prediction evolves:\n\ncurrent_word = \"wow\"\nfor (i in 1:5) {\n  predicted_word = next_word(current_word, ngrams)\n  cat(current_word, \"-&gt;\", predicted_word, \"\\n\")\n  current_word = predicted_word\n}\n\nwow -&gt; severance \nseverance -&gt; season \nseason -&gt; finale \nfinale -&gt; severance \nseverance -&gt; season \n\n\nIf you have played with this code, you might notice that the predictions can sometimes lead to repetitive or nonsensical sequences. This is a limitation of using simple n-gram models without additional context or smoothing techniques. We can explore by using trigrams to see if predictions improve:\n\n# Function to predict the next word based on a given two-word phrase using trigrams\nnext_word_trigram &lt;- function(phrase, trigrams_df) {\n    words &lt;- unlist(strsplit(phrase, \" \"))\n    if (length(words) != 2) {\n        stop(\"Please provide a two-word phrase.\")\n    }\n    matches &lt;- trigrams_df %&gt;%\n        separate(ngrams, into = c(\"w1\", \"w2\", \"w3\"), sep = \" \", remove = FALSE) %&gt;%\n        filter(w1 == words[1], w2 == words[2]) %&gt;%\n        pull(w3)\n    freq &lt;- table(matches)\n    nw &lt;- max(freq)\n    return(names(freq[freq == nw]))\n}\n\nTo use this function you would provide a two-word phrase, for instance ‚Äúbest show‚Äù:\n\ntype_any_phrase = \"best show\"\nnext_word_trigram(type_any_phrase, trigrams)\n\n[1] \"ever\" \"tv\"",
    "crumbs": [
      "Text Analysis",
      "N-grams and Collocations"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/ngrams.html#from-n-grams-to-collocations",
    "href": "chapters/2.TextAnalysis/ngrams.html#from-n-grams-to-collocations",
    "title": "N-grams and Word Sequences",
    "section": "From N-grams to Collocations",
    "text": "From N-grams to Collocations\nWhile n-grams capture all consecutive word sequences, not all of them are equally meaningful. Collocations are word combinations that occur together more frequently than would be expected by chance. They represent meaningful multi-word expressions like ‚Äústrong coffee,‚Äù ‚Äúmake a decision,‚Äù or in our data, perhaps ‚Äúplot twist‚Äù or ‚Äúcharacter development.‚Äù\nThe key difference: - N-grams: mechanical extraction of all consecutive words - Collocations: statistically significant word pairs that carry specific meaning\n\nIdentifying Collocations\nTo find collocations, we need to measure how ‚Äúassociated‚Äù two words are. One common metric is Pointwise Mutual Information (PMI), which compares how often words appear together versus how often we‚Äôd expect them to appear together if they were independent.\n\n\n\n\n\n\nNoteOther Collocation Metrics\n\n\n\n\n\nWhile we use PMI in this workshop, there are several other statistical measures commonly used to identify collocations:\n\nChi-square (œá¬≤): Tests the independence of two words by comparing observed vs.¬†expected frequencies. Higher values indicate stronger association.\nLog-likelihood ratio (G¬≤): Similar to chi-square but more reliable for small sample sizes. Commonly used in corpus linguistics.\nT-score: Measures the confidence in the association between two words. Less sensitive to low-frequency pairs than PMI.\nDice coefficient: Measures the overlap between two words‚Äô contexts. Values range from 0 to 1.\n\nEach metric has different strengths. PMI favors rare but strongly associated pairs, while t-score is more conservative and favors frequent collocations. The choice depends on your research goals and corpus characteristics.\n\n\n\nFirst, let‚Äôs separate our bigrams and count them:\n\nlibrary(tidyr)\n\n# Separate bigrams into individual words and count\nbigram_counts &lt;- ngrams %&gt;%\n  separate(ngrams, into = c(\"word1\", \"word2\"), sep = \" \", remove = FALSE) %&gt;%\n  count(word1, word2, sort = TRUE)\n\nhead(bigram_counts, 10)\n\n# A tibble: 10 √ó 3\n   word1     word2         n\n   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt;\n 1 season    finale     1793\n 2 severance season     1500\n 3 apple     tv          689\n 4 finale    severance   532\n 5 season    severance   378\n 6 severance finale      376\n 7 can       wait        181\n 8 best      show        159\n 9 second    season      159\n10 severance apple       156\n\n\nNow we‚Äôll calculate PMI for each bigram. PMI is calculated as:\n\\[\\text{PMI}(w_1, w_2) = \\log_2\\left(\\frac{P(w_1, w_2)}{P(w_1) \\times P(w_2)}\\right)\\]\nWhere:\n\n\\(P(w_1, w_2)\\) is the probability of the bigram occurring\n\\(P(w_1)\\) and \\(P(w_2)\\) are the probabilities of each word occurring independently\n\n\nlibrary(dplyr)\n\n# Calculate individual word frequencies\nword_freqs &lt;- comments %&gt;%\n  unnest_tokens(word, comments) %&gt;%\n  count(word, name = \"word_count\")\n\n# Total number of words in corpus\ntotal_words &lt;- sum(word_freqs$word_count)\n\n# Total number of bigrams\ntotal_bigrams &lt;- sum(bigram_counts$n)\n\n# Calculate PMI\ncollocations &lt;- bigram_counts %&gt;%\n  left_join(word_freqs, by = c(\"word1\" = \"word\")) %&gt;%\n  rename(word1_count = word_count) %&gt;%\n  left_join(word_freqs, by = c(\"word2\" = \"word\")) %&gt;%\n  rename(word2_count = word_count) %&gt;%\n  mutate(\n    # Probability of bigram\n    p_bigram = n / total_bigrams,\n    # Probability of each word\n    p_word1 = word1_count / total_words,\n    p_word2 = word2_count / total_words,\n    # PMI calculation\n    pmi = log2(p_bigram / (p_word1 * p_word2))\n  ) %&gt;%\n  arrange(desc(pmi))\n\nhead(collocations, 15)\n\n# A tibble: 15 √ó 9\n   word1      word2     n word1_count word2_count p_bigram p_word1 p_word2   pmi\n   &lt;chr&gt;      &lt;chr&gt; &lt;int&gt;       &lt;int&gt;       &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 abbott     elem‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n 2 abrams     ente‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n 3 accompani‚Ä¶ guid‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n 4 ace        disb‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n 5 acknowled‚Ä¶ empl‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n 6 activates  glas‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n 7 adams      stal‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n 8 affection  rain‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n 9 afternoon  wear‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n10 al         sc        1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n11 alfred     neum‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n12 alia       shaw‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n13 ambivalent rela‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n14 ampex      resu‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n15 amy        schu‚Ä¶     1           1           1  1.49e-5 1.37e-5 1.37e-5  16.3\n\n\nHigh PMI values indicate strong collocations, that means word pairs that appear together much more than chance would predict.\n\n\nVisualizing Collocations\nLet‚Äôs visualize the strongest collocations to see what meaningful phrases emerge from our Severance comments:\n\nlibrary(ggplot2)\n\n# Top 20 collocations by PMI\ntop_collocations &lt;- collocations %&gt;%\n  head(20) %&gt;%\n  unite(bigram, word1, word2, sep = \" \")\n\nggplot(top_collocations, aes(x = reorder(bigram, pmi), y = pmi)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 20 Collocations by PMI\",\n    x = \"Bigram\",\n    y = \"Pointwise Mutual Information\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nUsing Collocations for Smarter Prediction\nRemember our simple n-gram predictor that sometimes got stuck in loops? We can create a more ‚Äúintelligent‚Äù predictor using collocations instead of raw frequency counts. The idea is simple: instead of picking the most frequent next word, we pick the word with the highest PMI (strongest association).\n\n# Function to predict next word using collocation strength (PMI)\nnext_word_collocation &lt;- function(word, collocations_df, min_freq = 2) {\n    candidates &lt;- collocations_df %&gt;%\n        filter(word1 == word, n &gt;= min_freq, pmi &gt; 0) %&gt;%\n        arrange(desc(pmi))\n    \n    # Return the word with highest PMI, or NA if no matches\n    if (nrow(candidates) &gt; 0) {\n        return(candidates$word2[1])\n    } else {\n        return(NA)\n    }\n}\n\nLet‚Äôs compare the two approaches side by side:\n\n# Compare frequency-based vs. collocation-based prediction\ntest_word &lt;- \"mark\"\n\nfreq_prediction &lt;- next_word(test_word, ngrams)\ncolloc_prediction &lt;- next_word_collocation(test_word, collocations)\n\ncat(\"Frequency-based predictor:\", test_word, \"-&gt;\", freq_prediction, \"\\n\")\n\nFrequency-based predictor: mark -&gt; helly \n\ncat(\"Collocation-based predictor:\", test_word, \"-&gt;\", colloc_prediction, \"\\n\")\n\nCollocation-based predictor: mark -&gt; completing \n\n\nNow let‚Äôs run both predictors in a loop and see which produces more meaningful sequences:\n\n# Frequency-based prediction\ncurrent_word &lt;- \"wow\"\nfor (i in 1:10) {\n  predicted_word &lt;- next_word(current_word, ngrams)\n  cat(current_word, \"-&gt;\", predicted_word, \"\\n\")\n  current_word &lt;- predicted_word\n}\n\nwow -&gt; severance \nseverance -&gt; season \nseason -&gt; finale \nfinale -&gt; severance \nseverance -&gt; season \nseason -&gt; finale \nfinale -&gt; severance \nseverance -&gt; season \nseason -&gt; finale \nfinale -&gt; severance \n\ncurrent_word &lt;- \"wow\"\nfor (i in 1:10) {\n  predicted_word &lt;- next_word_collocation(current_word, collocations)\n  if (is.na(predicted_word)) {\n    cat(current_word, \"-&gt; (no strong collocation found)\\n\")\n    break\n  }\n  cat(current_word, \"-&gt;\", predicted_word, \"\\n\")\n  current_word &lt;- predicted_word\n}\n\nwow -&gt; intense \nintense -&gt; exploding \nexploding -&gt; head \nhead -&gt; contenders \ncontenders -&gt; tv \ntv -&gt; announces \nannounces -&gt; severance \nseverance -&gt; tvtime \ntvtime -&gt; (no strong collocation found)\n\n\nAs you can notice, both approaches are similar in structure, both are looking for the next word based on the current word. However, the collocation-based predictor leverages statistical associations between words, potentially leading to more contextually relevant predictions. This is an example of how different text analysis techniques can produce varying results based on the underlying data and methods used.",
    "crumbs": [
      "Text Analysis",
      "N-grams and Collocations"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/word_frequencies.html",
    "href": "chapters/2.TextAnalysis/word_frequencies.html",
    "title": "Basic word frequencies",
    "section": "",
    "text": "In this chapter, we will explore some basic techniques for analyzing text data.",
    "crumbs": [
      "Text Analysis",
      "Basic Word Frequencies"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/word_frequencies.html#importing-the-data",
    "href": "chapters/2.TextAnalysis/word_frequencies.html#importing-the-data",
    "title": "Basic word frequencies",
    "section": "Importing the Data",
    "text": "Importing the Data\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(tidytext)\n\n\n# Load the text data\ncomments &lt;- readr::read_csv(\"data/clean/comments_preprocessed.csv\") # Adjust the path to your data location\n\nExplore the first few rows of the dataset to understand its structure.\n\nhead(comments)\n\n# A tibble: 6 √ó 3\n   ...1 id      comments                                                        \n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;                                                           \n1     1 s1_0001 everyone telling watch severance nobody tryna watch apple       \n2     2 s1_0002 can quite explain show severance captivating far                \n3     3 s1_0003 full body clench entire second half severance season finale gri‚Ä¶\n4     4 s1_0004 one greatest movie series ever seen                             \n5     5 s1_0005 severance apple tv delivered best season finale seen years fire \n6     6 s1_0006 severance apple fire                                            \n\n\nWe can see that the dataset is imported as a tibble with three columns: ..1, id, and comments. We are going to focus on the comments column for our text analysis, but the id column can be useful for grouping or filtering the data by season.",
    "crumbs": [
      "Text Analysis",
      "Basic Word Frequencies"
    ]
  },
  {
    "objectID": "chapters/2.TextAnalysis/word_frequencies.html#word-frequency-analysis",
    "href": "chapters/2.TextAnalysis/word_frequencies.html#word-frequency-analysis",
    "title": "Basic word frequencies",
    "section": "Word Frequency Analysis",
    "text": "Word Frequency Analysis\nWord frequency analysis is one of the most fundamental techniques in text analysis. It helps us understand which words appear most often in our texts and can reveal important patterns about content, style, and themes.\n\nWord Counts\nWe can start by calculating the frequency of each word in our corpus. This involves tokenizing the text into individual words, counting the occurrences of each word, and then sorting them by frequency.\nTokenization is the process of breaking down text into smaller units, such as words or phrases. In this case, we will use the unnest_tokens() function from the tidytext package to tokenize our comments into words.\n\n\n\n\n\n\nNoteWhy not use strsplit()?\n\n\n\n\n\nWhile the strsplit() function can be used for basic tokenization, it lacks the advanced features provided by unnest_tokens(), such as handling punctuation, converting to lowercase, and removing stop words. Using unnest_tokens() ensures a more accurate and efficient tokenization process, especially for larger datasets.\n\n\n\n\n# Tokenizing the comments into words\ntokens &lt;- comments %&gt;%\n  unnest_tokens(word, comments)\n\nhead(tokens)\n\n# A tibble: 6 √ó 3\n   ...1 id      word     \n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    \n1     1 s1_0001 everyone \n2     1 s1_0001 telling  \n3     1 s1_0001 watch    \n4     1 s1_0001 severance\n5     1 s1_0001 nobody   \n6     1 s1_0001 tryna    \n\n\nNote that the resulting tokens tibble contains a column named word, which holds the individual words extracted from each comment.\nWith this tokenized data, we can now counting words. For instance, just simply counting the occurrences of each word:\n\n# Counting word frequencies\nword_counts &lt;- tokens %&gt;%\n  count(word, sort = TRUE) \n\nhead(word_counts)\n\n# A tibble: 6 √ó 2\n  word          n\n  &lt;chr&gt;     &lt;int&gt;\n1 severance  6040\n2 season     4480\n3 finale     2620\n4 show       1652\n5 tv         1060\n6 apple       890\n\n\nThis will give us a list of words along with their corresponding frequencies, sorted in descending order. We can also visualize the most common words using a bar plot or a word cloud.\n\n# Visualizing the top 20 most common words\ntop_words &lt;- word_counts %&gt;%\n  top_n(20)\n\nSelecting by n\n\nggplot(top_words, aes(x = reorder(word, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Top 20 Most Common Words\", x = \"Words\", y = \"Frequency\")\n\n\n\n\n\n\n\n\nWe can also create a word cloud to visualize word frequencies in a more engaging way.\n\n# Creating a word cloud\nlibrary(ggwordcloud)\n\nggplot(word_counts %&gt;% top_n(100), aes(label = word, size = n)) +\n  geom_text_wordcloud() +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  coord_fixed(ratio = 1) +\n  labs(title = \"Word Cloud\")\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nAs expected, even in a preprocessed corpus, some words become dominant due to their frequent usage. In this case, ‚Äúseverance‚Äù, ‚Äúseason‚Äù, and ‚Äúfinale‚Äù, pop up as the most frequent words. To get a more meaningful analysis, we can filter out these common words.\n\n# Filtering out common words for a more meaningful word cloud\ncommon_words &lt;- c(\"severance\", \"season\", \"appleTV\", \"apple\", \"tv\", \"show\", \"finale\", \"episode\") # you can expand this list as needed\n\nfiltered_word_counts &lt;- word_counts %&gt;%\n  filter(!word %in% common_words)\n\n# Creating a filtered word cloud\nggplot(filtered_word_counts %&gt;% top_n(100), aes(label = word, size = n)) +\n  geom_text_wordcloud() +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  coord_fixed(ratio = 1) +\n  labs(title = \"Filtered Word Cloud\")\n\nSelecting by n\n\n\n\n\n\n\n\n\n\nNow we have a more distributed word cloud that highlights other significant words in the corpus.\n\n\nWords by Season\nA simple but effective way to analyze text data is to compare word frequencies across different categories or groups. In this case, we can compare the word frequencies between different seasons of the show.\n\n# Filtering and creating word clouds by season\n\nseason_1_tokens &lt;- tokens %&gt;%\n  filter(grepl(\"^s1\", id)) %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(!word %in% common_words) %&gt;%\n  top_n(20)\n\nSelecting by n\n\nseason_2_tokens &lt;- tokens %&gt;%\n  filter(grepl(\"^s2\", id)) %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(!word %in% common_words) %&gt;%\n  top_n(20)\n\nSelecting by n\n\nlibrary(patchwork)\n\np1 &lt;- ggplot(season_1_tokens, aes(label = word, size = n)) +\n  geom_text_wordcloud(color = \"darkblue\") +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  coord_fixed(ratio = 1) +\n  labs(title = \"Season 1\")\n\np2 &lt;- ggplot(season_2_tokens, aes(label = word, size = n)) +\n  geom_text_wordcloud(color = \"darkred\") +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  coord_fixed(ratio = 1) +\n  labs(title = \"Season 2\")\n\np1 + p2\n\n\n\n\n\n\n\n\nWe can even select the 50 more frequent words per season and extract those that are unique to each season.\n\n# Finding unique words per season\ntop_50_s1 &lt;- season_1_tokens %&gt;%\n  top_n(50) %&gt;%\n  pull(word)\n\nSelecting by n\n\ntop_50_s2 &lt;- season_2_tokens %&gt;%\n  top_n(50) %&gt;%\n  pull(word)\n\nSelecting by n\n\nunique_s1 &lt;- setdiff(top_50_s1, top_50_s2)\nunique_s2 &lt;- setdiff(top_50_s2, top_50_s1)\n\nunique_s1_tokens &lt;- season_1_tokens %&gt;%\n  filter(word %in% unique_s1)\nunique_s2_tokens &lt;- season_2_tokens %&gt;%\n  filter(word %in% unique_s2)\n\n# Displaying unique word clouds for each season\np3 &lt;- ggplot(unique_s1_tokens, aes(label = word, size = n)) +\n  geom_text_wordcloud(color = \"lightblue\") +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  coord_fixed(ratio = 1) +\n  labs(title = \"Unique Words - Season 1\")\n\np4 &lt;- ggplot(unique_s2_tokens, aes(label = word, size = n)) +\n  geom_text_wordcloud(color = \"lightcoral\") +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  coord_fixed(ratio = 1) +\n  labs(title = \"Unique Words - Season 2\")\n\np3 + p4\n\n\n\n\n\n\n\n\nThis analysis allows us to see which words are more prominent in each season, providing insights into the themes and topics that were more relevant during those times.\nWith these basic text analysis techniques, we can start to uncover patterns and insights from our text data. Although simple, these methods are helpful to explore the content and structure of the text, setting the stage for more advanced analyses, or even informing about the quality of the pre-processing steps applied to the data.",
    "crumbs": [
      "Text Analysis",
      "Basic Word Frequencies"
    ]
  },
  {
    "objectID": "chapters/3.SentimentAnalysis/emotion.html",
    "href": "chapters/3.SentimentAnalysis/emotion.html",
    "title": "Emotion Detection",
    "section": "",
    "text": "Emotion detection is another NLP technique aimed at identifying and quantifying human emotions expressed in text, which builds directly on traditional sentiment polarity analysis focusing on capturing more nuanced emotional states. While polarity classification identifies whether a text expresses positive, negative, or neutral sentiment, it does not capture the specific type of emotion behind that sentiment. For example, two negative texts could express very different emotions‚Äîone might convey anger, while another reflects sadness. By extending polarity into multiple emotional dimensions, emotion detection provides more granular and more actionable insights into how people truly feel.\nWe will use the syuzhet package (more info) to to help us classify emotions detected in our dataset. The name ‚Äúsyuzhet‚Äù is inspired by the work of Russian Formalists Victor Shklovsky and Vladimir Propp, who distinguished between two aspects of a narrative: the fabula and the syuzhet. The fabula represents the chronological sequence of events, while the syuzhet refers to the way these events are presented or structured; the narrative‚Äôs technique or ‚Äúdevice.‚Äù In other words, syuzhet focuses on how the story (fabula) is organized and conveyed to the audience.\nThe syuzhet package implements the National Research Council Canada (NRC) Emotion Lexicon which maps words to basic emotions, in addition to polarity scores, allowing for fine-grained emotion scoring at the word, sentence, or document level.\nThis framework uses eight categories of emotions based on Robert Plutchik‚Äôs theory of the emotional wheel, a foundational model that illustrates the relationships between human emotions from a psychological perspective. Plutchik‚Äôs wheel identifies eight primary emotions: anger, disgust, sadness, surprise, fear, trust, joy, and anticipation. As illustrated in Figure ? below, these emotions are organized into four pairs of opposites on the wheel. Emotions positioned diagonally across from each other represent opposites, while adjacent emotions share similarities, reflecting a positive correlation.\n\n\n\nPlutchik‚Äôs wheel of emotions. Image from: Zeng, X., Chen, Q., Chen, S., & Zuo, J. (2021). Emotion label enhancement via emotion wheel and lexicon. Mathematical Problems in Engineering, 2021(1), 6695913. https://doi.org/10.1155/2021/6695913\n\n\nThe NRC Emotion Lexicon was developed as part of research into affective computing and sentiment analysis using a combination of manual annotation and crowdsourcing. Human annotators evaluated thousands of words, indicating which emotions were commonly associated with each word. This method ensured that the lexicon captured human-perceived emotional associations, rather than relying solely on statistical co-occurrences in text.\nSince its release, the NRC Emotion Lexicon has become a widely used resource in computational social science, marketing analytics, and text mining, because it allows researchers to move beyond simple positive/negative polarity to fine-grained emotion detection, making it possible to analyze the emotional content of text at scale.\nYou may explore NRC‚Äôs lexicon Tableau dashboard to explore words associated with each emotion category:\n\nNow that we have a better understanding of this package, let‚Äôs get back to business and perform emotion detection to our data.\n\nEmotion Detection with Syuzhet‚Äôs NRC Lexicon\n\nDetecting Emotions per Comment/Sentence\nsentences &lt;- get_sentences(comments$comments)\n\n\nCompute Emotion Scores per Sentence\nemotion_score &lt;- get_nrc_sentiment(sentences)\nThe get_nrc_sentiment() function assigns emotion and sentiment scores (based on the NRC lexicon) to each sentence. Each sentence gets numeric values (0 or 1) for the eight emotions to represent their absence or presence. The output also includes positive and negative sentiment scores:\n\n\n\nReview Summary of Emotion Scores\nLet‚Äôs now compute basic statistics (min, max, mean, etc.) for each emotion column and get an overview of how frequent or strong each emotion is on our example dataset.\nsummary(emotion_score)\nThis step should generate the following output:\n\nBased on the results the overall emotion in these comments leans heavily toward sadness, which scored the highest average (1.236). It looks like sadness and trust are the most common feelings, since they‚Äôre the only ones with a median score of 1.000, meaning at least half the comments contained words for them.\nOn the flip side, Disgust was the rarest emotion, with the lowest average (0.145). It‚Äôs also worth noting that while Sadness and Trust are the most common, a few comments really went off the rails with Trust (47.000), Anger (44.000), and Fear (37.000), hitting the highest extreme scores.\n\n\nRegroup with comments and IDs\nAfter computing scores for emotions, we want to link them back to its original comment and ID.\ncomments$comments &lt;- sentences\nemotion_data &lt;- bind_cols(comments, emotion_score)\nbind_cols() merges the original comments data frame with the new emotion_score table.\n\n\nSummarize Emotion Counts Across All Sentences\nNow, let‚Äôs count how many times each emotion appears overall.\nemotion_summary &lt;- emotion_data %&gt;%\n  select(anger:trust) %&gt;% # get only the emotion columns\n  summarise(across(everything(), sum)) %&gt;% # sum counts \n  pivot_longer(\n    cols = everything(), \n    names_to = \"emotion\", \n    values_to = \"count\"\n    ) %&gt;% # long format for easy plotting\n  arrange(desc(count)) # sort emotions\n\n\n\nPlot the Overall Emotion Distribution\nggplot(emotion_summary, aes(x = emotion, y = count, fill = emotion)) +\n  geom_col(show.legend = FALSE) +              # Bar plot for emotion counts\n  geom_text(aes(label = count), hjust = -0.2, size = 2) +  # Add count labels\n  scale_fill_manual(values = brewer.pal(10, \"Paired\")) +   # Color palette\n  theme_minimal(base_size = 12) +             # Clean theme\n  labs(title = \"Overall Emotion Distribution\",\n       x = \"Emotion\", y = \"Total Count\") +    # Titles and axis labels\n  coord_flip()                                # Flip axes for readability\n\n\n\nAdd a ‚ÄúSeason‚Äù Variable (Grouping) and Summarize\nLet‚Äôs now add a new column called season by looking at the ID pattern ‚Äî for example, s1_ means season 1 and s2_ means season 2. This makes it easy to compare the emotional tone across seasons.\nemotion_seasons &lt;- emotion_data %&gt;%\n  mutate(season = ifelse(grepl(\"^s1_\", id), \"s1\",\n                  ifelse(grepl(\"^s2_\", id), \"s2\", NA)))\nTime to aggregates the total count of each emotion within each season.\n# Aggregate emotion counts per season\nemotion_by_season &lt;- emotion_seasons %&gt;%\n  group_by(season) %&gt;%\n  summarise(\n    across(anger:positive, ~sum(., na.rm = TRUE))\n  )\n\n\nPlotting the Data\nComparing emotions by season:\nemotion_long &lt;- emotion_by_season %&gt;%\n  pivot_longer(cols = anger:positive, names_to = \"emotion\", values_to = \"count\")\n\nggplot(emotion_long, aes(x = reorder(emotion, -count), y = count, fill = season)) +\n  geom_col(position = \"dodge\") + # separates bars for clarity\n  geom_text(aes(label = count), hjust = -0.2, size = 2) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal(base_size = 12) +\n  labs(title = \"Emotion Distribution by Season\", \n       x = \"Emotion\", y = \"Total Count\", fill = \"Season\") +\n  coord_flip()\n\nNow, let‚Äôs explore to see which emotions tend to occur together, revealing patterns of emotional co-occurrence in the text.\n# Select only emotion columns (excluding overall positive/negative sentiment)\nemotion_matrix &lt;- emotion_data %&gt;%\n  select(anger:trust)\n\n# Compute the correlation matrix for emotions\n# Pearson correlation shows how strongly two emotions co-occur\nco_occurrence &lt;- cor(emotion_matrix, method = \"pearson\")\n\n# Remove diagonal values to avoid coloring the perfect self-correlation\ndiag(co_occurrence) &lt;- NA\n\n# Convert the correlation matrix to long format for ggplot\nco_occurrence_long &lt;- as.data.frame(as.table(co_occurrence))\ncolnames(co_occurrence_long) &lt;- c(\"emotion1\", \"emotion2\", \"correlation\")\n\n# Plot the co-occurrence heatmap\nggplot(co_occurrence_long, aes(x = emotion1, y = emotion2, fill = correlation)) +\n  geom_tile(color = \"white\") +  # draw grid tiles\n  scale_fill_gradient2(\n    mid = \"white\", high = \"red\", midpoint = 0,\n    limits = c(0, 1), na.value = \"grey95\", name = \"Correlation\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # rotate x-axis labels\n  labs(\n    title = \"Emotion Co-occurrence Heatmap\",\n    x = \"Emotion\",\n    y = \"Emotion\"\n  )\nAfter running the script we should get the following heat map:\n\nBased on these results, Overall, the emotional picture is pretty interconnected. It looks like the negative emotions‚ÄîSadness, Fear, Anger, and Disgust‚Äîare more tightly linked, meaning when people express one of these, they usually express the others too. In other words, they often show up together in the same comments.\nWhile we‚Äôve only scratched the surface of this particular dataset, the steps we‚Äôve completed‚Äîfrom calculating basic sentiment scores to visualizing the co-occurrence of emotions‚Äîhave demonstrated the power of sentiment and emotion detection. You now have the foundational skills to convert unstructured text into actionable data, allowing you to understand the polarity (positive/negative) and specific emotional landscape of any textual dataset.\n\n\nSaving our work\nAfter performing all the calculations and visualizations, it‚Äôs important to save the results so they can be reused or shared.\nwrite_csv(emotion_data, \"output/sentiment_emotion_results.csv\")",
    "crumbs": [
      "Sentiment Analysis",
      "Emotion Detection"
    ]
  },
  {
    "objectID": "chapters/3.SentimentAnalysis/polarity.html",
    "href": "chapters/3.SentimentAnalysis/polarity.html",
    "title": "Polarity Classification",
    "section": "",
    "text": "Polarity classification is a fundamental aspect of sentiment analysis to measure the overall emotional tone expressed in text data which can be categorized as positive, neutral or negative.\nMost models assign ‚Äúsentiment scores‚Äù with values ranging from -1 to +1 to represent the intensity of the sentiment, being scores closer to -1 considered negative, those closer to 0 neutral and +1 positive.\n\nWe will be using the package sentimentr (more info) to compute polarity classification and attribute sentiment scores to the posts included in our dataset.\nTraditional sentiment analysis techniques assign polarity by matching words against dictionaries labeled as ‚Äúpositive,‚Äù ‚Äúnegative,‚Äù or ‚Äúneutral.‚Äù While straightforward, this approach is overly simplistic: it ignores context and flattens the richness of our syntactically complex, lexically nuanced language, that transcends individual words. The sentimentr package extends lexicon-based methods by accounting for valence shifters; words that subtly alter sentiment.\nThe package includes 130 valence shifters that can reverse or modulate the sentiment indicated by standard dictionaries. These fall into four main categories: negators (e.g., not, can‚Äôt), amplifiers (e.g., very, really, absolutely, totally, certainly), de-amplifiers or down-toners (e.g., barely, hardly, rarely, almost), and adversative conjunctions (e.g., although, however, but, yet, that being said). This refinement is important because simple dictionary lookups miss the nuanced meaning.\nIn summary, each word in a sentence is checked against a dictionary of positive and negative words, like the Jockers dictionary in the lexicon package. Words that are positive get a +1, and words that are negative get a -1, which are called polarized words. Around each polarized word, we look at the nearby words (usually four before and two after) to see if they change the strength or direction of the sentiment. This group of words is called a polarized context cluster. Words in the cluster can be neutral, negators (like ‚Äúnot‚Äù), amplifiers (like ‚Äúvery‚Äù), or de-amplifiers (like ‚Äúslightly‚Äù). Neutral words don‚Äôt affect the sentiment but still count toward the total word count.\nThe main polarized word‚Äôs sentiment is then adjusted by the surrounding words. Amplifiers make it stronger, de-amplifiers make it weaker, and negators can flip the sentiment. Multiple negators can cancel each other out, like ‚Äúnot unhappy‚Äù turning positive.\nWords like ‚Äúbut,‚Äù ‚Äúhowever,‚Äù and ‚Äúalthough‚Äù also influence the sentiment. Words before these are weighted less, and words after them are weighted more because they signal a shift in meaning. Finally, all the adjusted scores are combined and scaled by the sentence length to give a final sentiment score for the sentence.\nWith this approach, we can explore more confidently whether the show‚Äôs viewers felt positive, neutral, or negative about it.\n\nComputing Polarity with Sentiment R (Valence Sifters Capability)\n\nCalculating sentiment scores\nHere we‚Äôre using the sentiment_by() function which looks at each comment and calculates a sentiment score representing how positive or negative comments are.\nsentiment_scores &lt;- sentiment_by(comments$comments)\n\n\n\nSentiment Scores Output\n\n\nSo after running this, we get a new object called sentiment_scores with the average sentiment for every comment. Can you guess why the SD column is empty? A single data point (sentence/row) does not have a standard deviation by itself.\n\n\nAdding those scores back to our dataset\nNow we‚Äôre using the dplyr package to make our dataset more informative. We take our comments dataset, and with mutate(), we add two new columns: score and sentiment label. The little rule inside case_when() decides what label to give. The small buffer around zero (¬±0.1) helps us avoid overreacting to tiny fluctuations.\npolarity &lt;- comments %&gt;%\n  mutate(score = sentiment_scores$ave_sentiment,\n         sentiment_label = case_when(\n           score &gt; 0.1  ~ \"positive\",\n           score &lt; -0.1 ~ \"negative\",\n           TRUE         ~ \"neutral\"\n         ))\nLet‚Äôs now take a look at the sentiment_scores data frame:\n\n\n\nSentiment Scores with Polarity Results\n\n\nTo get a sense of the overall mood of our dataset let‚Äôs run:\ntable(polarity$sentiment_label)\n\n\n\nOverall Polarity Count\n\n\nOverall, the majority of viewers reacted positively to the show, with positive opinions more than double the negative ones, indicating a generally favorable reception. However, this is only part of the story‚Äîpositive sentiment can range from mildly favorable to very enthusiastic. To better visualize the full distribution of opinions, a histogram is presented below.\n\n\n\nPlotting Scores\nNext, let‚Äôs plot some results and histograms to check the distribution for the scores:\n# Visualize\nggplot(polarity, aes(x = score)) +\n  geom_histogram(binwidth = 0.1, fill = \"skyblue\", color = \"white\") +\n  theme_minimal() +\n  labs(title = \"Sentiment Score Distribution\", x = \"Average Sentiment\", y = \"Count\")\n\n\n\nPolarity Distribution\n\n\nThis histogram suggests that the overall sentiment toward the Severance show was mostly neutral to slightly positive. This suggests that most viewers are expressing their opinions in a measured, nuanced, or factual manner, rather than with intense emotional language (either extremely positive or negative).\nWe can also break the data down by season to compare how audience opinions vary over each season finale:\n# Extract season info (s1, s2) into a new column\npolarity_seasons &lt;- mutate(polarity,\n                           season = str_extract(id, \"s\\\\d+\"))\n\n# Histogram comparison by season, using Density\nggplot(polarity_seasons, aes(x = score, fill = season)) +\n  geom_histogram(aes(y = after_stat(density)),  \n                 binwidth = 0.1, \n                 position = \"dodge\", \n                 color = \"white\") +\n  theme_minimal() +\n  labs(title = \"Sentiment Score Distribution by Season (Normalized)\", \n       x = \"Average Sentiment Score (Polarity)\", \n       y = \"Density (Proportion of Comments)\") + \n  scale_fill_brewer(palette = \"Set1\")\n\nThis plotting approach usind density allows us to directly compare the heights of the bars to determine which season had a higher proportion of comments at a specific polarity score, regardless of the total number of comments for that season.\nThis result let us infer that there were differences in audience reception and reaction between the two season finales. Overall, the S1 finale resulted in a strongly positive audience reaction, while the S2 finale led to a more mixed, analytical, and critical conversation, with significantly more proportional negative sentiment than S1.\n\n\nSaving Things\n# Save results\nwrite_csv(polarity, \"output/polarity_results.csv\")\nWe could have spent more time refining these plots, but this is sufficient for our initial exploration. In pairs, review the plots and discuss what they reveal about viewers‚Äô perceptions of the Severance show.\nWell, that‚Äôs only part of the story. Now we move on to emotion detection to discover what else we can learn from the data.",
    "crumbs": [
      "Sentiment Analysis",
      "Polarity Classification"
    ]
  }
]