[
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html",
    "href": "chapters/1.Preprocessing/02_normalization.html",
    "title": "Normalization & Noise Reduction",
    "section": "",
    "text": "To perform accurate and reliable analysis, we need to ‚Äútake out the garbage‚Äù first by preprocessing the text to clean, standardize, and structure the input data. This reduces noise and improves the model‚Äôs accuracy.\nBelow we use another analogy to represent the impact of noise in the data analysis outcome. Below we use another analogy to illustrate the impact of noise on data analysis outcomes. Imagine a tree that is slowly dying. On the surface, its leaves may still appear green, but closer inspection reveals branches that are brittle, bark that is cracking, and roots that are struggling to find nourishment. If we only focus on the healthy-looking leaves, we might draw a misleading conclusion about the tree‚Äôs overall condition. Similarly, in text analysis, raw data often contains ‚Äúnoise,‚Äù such as irrelevant words, inconsistent formatting, or errors, which can obscure meaningful patterns. If we feed this noisy data directly into an analysis, the results can be skewed, incomplete, or misleading, just as judging the tree‚Äôs health by its leaves alone would be.\nJust as a gardener would prune dead branches, enrich the soil, and care for the roots to revive the tree, data analysts perform preprocessing steps to clean, standardize, and structure the text. By removing noise and focusing on the core content, we give the analysis the best chance to reveal true insights, uncover trends, and support reliable conclusions. In short, the quality of our ‚Äúdata garden‚Äù directly determines the health of the insights it produces.\n\nAs we‚Äôve seen, the main goal of normalization is to remove irrelevant punctuation and content, and to standardize the data in order to reduce noise. Below are some key actions we‚Äôll be performing during this workshop:\n\n\n\n\n\n\n\nAction\nWhy it matters?\n\n\n\n\nRemove URLs\nURLs often contain irrelevant noise and don‚Äôt contribute meaningful content for analysis.\n\n\nRemove Punctuation & Symbols\nPunctuation marks and other symbols including those extensively used in social media for mentioning (@) or tagging (#) rarely adds value in most NLP tasks and can interfere with tokenization (as we will cover in a bit) or word matching.\n\n\nRemove Numbers\nNumbers can be noise in most contexts unless specifically relevant (e.g., in financial or medical texts) don‚Äôt contribute much to the analysis. However, in NLP tasks they are considered important, there might be considerations to replace them with dummy tokens (e.g.¬†&lt;NUMBER&gt;), or even converting them into their written form (e.g, 100 becomes one hundred).\n\n\nNormalize Whitespaces\nEnsures consistent word boundaries and avoids issues during tokenization or frequency analysis.\n\n\nConvert to Lowercase\nPrevents case sensitivity from splitting word counts due to case variations (e.g., ‚ÄúAppleTV‚Äù ‚â† ‚ÄúAPPLETV‚Äù ‚â† ‚ÄúappleTV‚Äù ‚â† ‚Äúappletv‚Äù), improving model consistency.\n\n\nConvert Emojis to Text\nEmojis play a unique role in text analysis, as they often convey sentiment. Rather than removing them, we will convert them into their corresponding text descriptions.\n\n\n\n\n\n\n\n\n\nNoteüß† Knowledge Check\n\n\n\nIn pairs or groups of three, identify the techniques you would consider using to normalize and reduce noise in the following sentence:\n‚ÄúOMG!! üò± I can‚Äôt believe it‚Ä¶ This is CRAZY!!! #unreal ü§Ø‚Äù\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nHow many techniques could you identify? Bingo if you have spotted all four!\nAfter applying them the sentence should look like:\nomg [face scream in fear] I can not believe it this is crazy unreal [exploding head]\n\n\n\n\n\nA caveat when working with emojis is that they are figurative and highly contextual. Also, there may be important generational and cultural variability in how people interpret them. For example, some countries may use the Folded Hands Emoji (üôè) as a sign of thank you where others may seem as religious expression. Also, some may use it in a more positive way as gratitude, hope or respect, or in a negative context, where they might be demonstrating submission or begging.\nYou might have noticed based on the example above that emojis are converted to their equivalent CLDR (common, human-readable name) based on this emoji unicode list, which are not as nuanced and always helpful to detect sentiment. While not always perfect, that is a required step to normalize the data.",
    "crumbs": [
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#dealing-with-contractions",
    "href": "chapters/1.Preprocessing/02_normalization.html#dealing-with-contractions",
    "title": "Normalization & Noise Reduction",
    "section": "Dealing with Contractions",
    "text": "Dealing with Contractions\nAnother important step is to properly handle contractions. In everyday language, we often shorten words: can‚Äôt, don‚Äôt, it‚Äôs. These make speech and writing flow more easily, but they can cause confusion for Natural Language Processing (NLP) models. Expanding contractions, such as changing can‚Äôt to cannot or it‚Äôs to it is, helps bring clarity and consistency to the text because NLP models treat don‚Äôt and do not as completely different words, even though they mean the same thing. Also, words like cant, doesnt, and whats lose their meaning. Expanding contractions reduces this inconsistency and ensures that both forms are recognized as the same concept. Expanding it to is not happy makes the negative sentiment explicit, which is especially important in tasks like sentiment analysis.\nSo, while it may seem like a small step, it often leads to cleaner data, leaner models, and more accurate results. First, however, we need to ensure that apostrophes are handled correctly. It‚Äôs not uncommon to encounter messy text where nonstandard characters are used in place of the straight apostrophe (‚Äô). Such inconsistencies are very common and can disrupt contraction expansion.\n\n\n\n\n\n\n\n\nCharacter\nUnicode\nNotes\n\n\n\n\n'\nU+0027\nStandard straight apostrophe, used in most dictionaries\n\n\n‚Äô\nU+2019\nRight single quotation mark (curly apostrophe)\n\n\n‚Äò\nU+2018\nLeft single quotation mark\n\n\n º\nU+02BC\nModifier letter apostrophe\n\n\n`\nU+0060\nGrave accent (sometimes typed by mistake)\n\n\n\nAlright, let‚Äôs go back to our worksheet to get our hands ‚Äúdirty‚Äù with some cleaning and normalization, helping us make it more normalized, consistent, and ready for analysis.\n\n\n\n\n\n\nCautionThe Order Matters!\n\n\n\nWhen performing text normalization and noise reduction, the order of steps matters because each transformation changes the text in a way that can affect subsequent steps. Doing things in a different order can lead to different results, and sometimes even incorrect or unexpected outcomes. For example, if we remove punctuation before expanding contractions, \"can't\" might turn into \"cant\" instead of \"cannot\", losing the correct meaning.",
    "crumbs": [
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#handling-apostrophes",
    "href": "chapters/1.Preprocessing/02_normalization.html#handling-apostrophes",
    "title": "Normalization & Noise Reduction",
    "section": "1. Handling Apostrophes",
    "text": "1. Handling Apostrophes\nThis step helps clean up text by making sure all apostrophes are consistent, rather than a mix of fancy Unicode versions. Apllying it to the text column in our comments dataset should look like. In this case, the pattern ‚Äú[‚Äô‚Äò º]‚Äù looks for several different kinds of apostrophes and backticks‚Äîlike the left and right single quotes, the modifier apostrophe, and the backtick. Each of those gets replaced with a simple, standard apostrophe (‚Äô`).",
    "crumbs": [
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#expanding-contractions",
    "href": "chapters/1.Preprocessing/02_normalization.html#expanding-contractions",
    "title": "Normalization & Noise Reduction",
    "section": "2. Expanding Contractions",
    "text": "2. Expanding Contractions\nLet‚Äôs first make sure that words like ‚Äúdon‚Äôt‚Äù become ‚Äúdo not‚Äù.",
    "crumbs": [
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#converting-to-lowercase",
    "href": "chapters/1.Preprocessing/02_normalization.html#converting-to-lowercase",
    "title": "Normalization & Noise Reduction",
    "section": "3. Converting to Lowercase",
    "text": "3. Converting to Lowercase\nHaving all text converted to lowercase will be our next step, using the mutate function we will add the following code which will create a new text_lower column:",
    "crumbs": [
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#removing-urls",
    "href": "chapters/1.Preprocessing/02_normalization.html#removing-urls",
    "title": "Normalization & Noise Reduction",
    "section": "4. Removing URLs",
    "text": "4. Removing URLs\nWe have a few URLs in our dataset and because they vary in format (e.g., http://, https://, or www.), we need to provide a regular expression that can take care of these variations and save it as text_nourl.\n#FIXME: ENTER NEW CODE AND CHECK CORRESPONDENT EXPLANATION BELOW\nBreaking it down:\n\nhttp[s]?:// : Matches http:// or https://, where [s]?: means optional ‚Äús‚Äù (so it matches both http and https).\n[^\\\\s,]+: where [^‚Ä¶] is a negated character class, meaning ‚Äúmatch anything not in here‚Äù, \\\\s refers to whitespace (space, tab, newline), and , to match one or more characters that are not spaces or commas, that in combination with the above, matches the entire URL up to a space or comma.\n| : Means OR in regex. So it matches either the first pattern or the second.\nwww\\\\.[^\\\\s,]+ : Matches URLs that start with www. and continue until a space or comma.",
    "crumbs": [
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#removing-mentions-handling-extra-space",
    "href": "chapters/1.Preprocessing/02_normalization.html#removing-mentions-handling-extra-space",
    "title": "Normalization & Noise Reduction",
    "section": "5. Removing Mentions & Handling Extra Space",
    "text": "5. Removing Mentions & Handling Extra Space\nContinuing with our workflow, we will now handle direct mentions and usernames in our dataset, as they do not contribute relevant information to our analysis. We will use a function to replace all occurrences of usernames preceded by an @ symbol.\n#FIXME: ENTER NEW CODE",
    "crumbs": [
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#cleaning-punctuation-numbers-elongation-and-extra-spaces-and-hidden-characters",
    "href": "chapters/1.Preprocessing/02_normalization.html#cleaning-punctuation-numbers-elongation-and-extra-spaces-and-hidden-characters",
    "title": "Normalization & Noise Reduction",
    "section": "6. Cleaning Punctuation, Numbers, Elongation and Extra Spaces and Hidden Characters",
    "text": "6. Cleaning Punctuation, Numbers, Elongation and Extra Spaces and Hidden Characters\nAlright, time to remove punctuation, numbers and repeated letters used for emphasis, that might affect our analysis (e.g.¬†‚ÄúAmaaaazing‚Äù, ‚ÄúLoooove‚Äù) and take care of some extra spaces.\nFIXME: ENTER NEW CODE\nThe punctuation regular expression:\n\n[:punct:]: matches all standard punctuation characters (e.g., ! ‚Äù # $ % & ‚Äô ( ) * + , - . / : ; &lt; = &gt; ? @ [ ¬†] ^ _ { | } ~).\n‚Äú‚Äù‚Äò‚Äô‚Äì‚Äî‚Ä¶ : matches special Unicode punctuation, like curly quotes, en-dash, em-dash, ellipsis.\n|: matches the literal pipe symbol.\n[:digit:]: matches any digit (0‚Äì9).\nFor the collapsing part, (.): captures any single character and stores it in a capture group.\n\\\\1: refers back to the first captured character.\n{2,}: matches 2 or more consecutive occurrences of that character.\n\nTogether, (.)\\\\1{2,} matches any character repeated three or more times in a row and str_replace_all replaces the repeated sequence with just one occurrence of the character.\n\n\n\n\n\n\nTipGet Help with Regex\n\n\n\nTesting regular expressions is essential for accuracy and reliability, since complex patterns often produce unexpected results. Careful testing ensures your regex matches the intended text, rejects invalid inputs, and performs efficiently, while also revealing potential bugs before they impact your system. To make testing more effective, use tools like Regex101 or the Coder Pad cheatsheet or and be sure to check tricky border cases that might otherwise slip through.\n\n\nWe have created multiple intermediate columns to facilitate the inspection of each step we performed, but we could have streamlined the pipeline with a more slim approach, like the one below:\n#FIXME: ADD NEW CODE\nFIXME: ADD OUTPUT SCREENSHOT",
    "crumbs": [
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/02_normalization.html#convert-emojis-to-text",
    "href": "chapters/1.Preprocessing/02_normalization.html#convert-emojis-to-text",
    "title": "Normalization & Noise Reduction",
    "section": "7. Convert Emojis to Text",
    "text": "7. Convert Emojis to Text\nOkay, now we‚Äôll convert emojis into their text descriptions to make them machine-readable, using the emoji package to help with this step:\n# Load the emoji dictionary\n#FIXME: ADD NEW CODE\nHave you had a chance to look at the emoji dictionary we loaded into our RStudio environment? It‚Äôs packed with more emojis and some surprising meanings than you might expect.\n# Define a function to replace emojis in text with their corresponding names.\nFIXME: ENTER NEW CODE\nWith emojis taken care of, we can now move on to the next preprocessing step: tokenization.\n\n\n\n\n\n\nNoteüìë Suggested Readings\n\n\n\nBai, Q., Dan, Q., Mu, Z., & Yang, M. (2019). A systematic review of emoji: Current research and future perspectives. Frontiers in psychology, 10, https://doi.org/10.3389/fpsyg.2019.02221\nGraham, P. V. (2024). Emojis: An Approach to Interpretation. UC L. SF Commc‚Äôn and Ent. J., 46, 123. https://repository.uclawsf.edu/cgi/viewcontent.cgi?article=1850&context=hastings_comm_ent_law_journal",
    "crumbs": [
      "Normalization & Noise Reduction"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/03_tokenization.html",
    "href": "chapters/1.Preprocessing/03_tokenization.html",
    "title": "Word Tokenization",
    "section": "",
    "text": "Tokenization in NLP differs from applications in security and blockchain. It corresponds to the action of breaking down text into smaller pieces (aka tokens). It is a foundational process in the digital world, allowing machines to interpret and analyze large volumes of text data. By dividing text into smaller, more manageable units, it enhances both the efficiency and accuracy of data processing.\nText can be tokenized into sentences, word, subwords or even characters, depending on project goals and analysis plan. Here is a summary of these approaches:\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\nCommon Use Cases\n\n\n\n\nSentence Tokenization\nSplits text into individual sentences\n\"I love NLP. It's fascinating!\" ‚Üí [\"I love NLP.\", \"It's fascinating!\"]\nIdeal for tasks like summarization, machine translation, and sentiment analysis at the sentence level\n\n\nWord Tokenization\nDivides text into individual words\n\"I love NLP\" ‚Üí [\"I\", \"love\", \"NLP\"]\nWorks well for languages with clear word boundaries, such as English\n\n\nCharacter Tokenization\nBreaks text down into individual characters\n\"NLP\" ‚Üí [\"N\", \"L\", \"P\"]\nUseful for languages without explicit word boundaries or for very fine-grained text analysis\n\n\nSubword Tokenization\nDecomposes words into smaller parts, like prefixes, suffixes, or common morphemes (the smallest units of meaning in a language)\n\"subword tokenization\" ‚Üí [\"sub\", \"word\", \"token\", \"ization\"]\nEffective for handling rare or unknown words and languages with complex word formation\n\n\n\nSome might recall, that along with the popularization and excitement around ChaGPT, there were also a few warnings about the LLMs failing in answering correctly how many ‚Äúr‚Äù letters does the word strawberry have. Can you guess why?\n\n\n\nImage from: OpenAI Community Forum [829618]\n\n\nAlthough this issue has been resolved in later versions of the model, it was originally caused by subword tokenization. In this case, the tokenizer would split ‚Äústrawberry‚Äù into ‚Äúst,‚Äù ‚Äúraw,‚Äù and ‚Äúberry.‚Äù As a result, the model would incorrectly count the letter ‚Äúr‚Äù only within the ‚Äúberry‚Äù token. This illustrates how the tokenization approach directly affects how words are segmented and how their components are interpreted by the model.\nWhile this is beyond the scope of the workshop, it‚Äôs important to note that some advanced AI models use neural networks to dynamically determine token segmentation. Rather than relying on fixed rules, these models can adapt based on the contextual cues within the text. However, tokenization remains inherently limited by the irregular, organic, and often unpredictable nature of human language.\nLet‚Äôs go back to our dataset and apply tokenization:\n#FIXME: ADD NEW CODE\nWe‚Äôve just tokenized each comment (or row of text) in our dataset into individual tokens, giving us a total of 124,864 tokens!\nIf you‚Äôve looked at the token output, you may have noticed that some of these tokens are common but less meaningful words, like the, a, or of. Don‚Äôt worry; we‚Äôll take care of those in the next step.",
    "crumbs": [
      "Word Tokenization"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/05_lemmatization.html",
    "href": "chapters/1.Preprocessing/05_lemmatization.html",
    "title": "Lemmatization",
    "section": "",
    "text": "Also known as word reduction, lemmatization is the process of transforming words into their base or dictionary form (lemma) to identify similarities in meaning and usage across different contexts. Take the word ‚Äúrun‚Äù as an example. It can appear in various forms like ‚Äúran‚Äù, ‚Äúruns‚Äù, ‚Äúrunning‚Äù, and ‚Äúrunner‚Äù. But the variations don‚Äôt stop there, as it includes complex forms like ‚Äúoutrun‚Äù, ‚Äúoverrun‚Äù, or ‚Äúunderrun‚Äù. These variations make it challenging for computers to process natural language effectively unless they can recognize that these words are related. That‚Äôs where lemmatization comes in; it helps machines group related words together by reducing them to a common root present in the dictionary, enabling better understanding and analysis of text.\nYou might be wondering: what about stemming? Stemming is a simpler but more aggressive process that removes prefixes and suffixes to reduce words to their root forms. It is generally considered less precise than lemmatization because it can sometimes produce meaningless words. For example, while ‚Äúran‚Äù and ‚Äúruns‚Äù would correctly stem to ‚Äúrun,‚Äù the word ‚Äúrunning‚Äù would be reduced to ‚Äúrunn.‚Äù\nFor this reason, we will stick with lemmatization and skip stemming in our pipeline. That said, if you need to process very large volumes of text and want a faster, more efficient approach, stemming could be a reasonable alternative.\nAn important thing to consider is that we look into words as separate units (tokens) as we saw in the previous episode. For example, think about the word ‚Äúleaves‚Äù. That could both represent the plural of the noun ‚Äúleaf‚Äù or the verb in third person for the word ‚Äúleave‚Äù. That is a good reminder of always remember to apply part of speech (POS) because lemmatization algorithms utilize a lexicon with linguistic rules based on pre-determined tags to avoid misinterpretation.\n\n\n\n\n\n\nNoteüß† Knowledge Check\n\n\n\nIn pairs or groups of three, apply lemmatization to the following sentence. Identify the base forms (lemmas) of each word:\nCats chasing mice running quickly across gardens.\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nHow many words did you successfully lemmatize? Bingo if you have identified all key lemmas!\nAfter applying lemmatization, the sentence should look like:\ncat chase mouse run quickly across garden\nNote: Adverbs and prepositions usually remain unchanged because they are already in their simplest dictionary form and do not have a more basic lemma.\n\n\n\n\n\nAlright, back to our pipeline, we will now convert words to their dictionary form, remove any remaining noise, and finalize our dataset for analysis.\n# Lemmatize\nfixme enter new code\n&lt;FIXME: ENTER OUTPUT&gt;\nDid you notice the new column added to the tokens_nostop data frame? If you haven‚Äôt yet, take a moment to check it out. It shows how some words have been converted to their dictionary or base forms.fixme: enter With that done, we can finally move on to the next chapter. But first, as a last step, we will save the file:\n# Save to CSV\nwrite_csv(preprocessed, \"preprocessed.csv\")",
    "crumbs": [
      "Lemmatization"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu ) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu",
    "crumbs": [
      "About RDS"
    ]
  },
  {
    "objectID": "about.html#ways-we-can-help-you",
    "href": "about.html#ways-we-can-help-you",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu ) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu",
    "crumbs": [
      "About RDS"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text Analysis with R",
    "section": "",
    "text": "Authors: Renata Curty  and Jairo Melo \nTODO: Add summary\n\n\nThese lessons are hands-on and are designed to be followed with R and RStudio open. Before starting, please ensure you have the following software installed:\n\nR: We recommend R version 4.3 or newer. Download from CRAN.\nRStudio: We recommend RStudio version 2023.12 or newer. Download from Posit‚Äôs website.\n\n\n\n\n\n\n\nNoteHow to update R and RStudio\n\n\n\n\n\nCheck your versions\n\nRStudio:\n\nOn Mac: Go to RStudio -&gt; About RStudio.\nOn Windows: Go to Help -&gt; About RStudio.\n\nR: In the R console, run:\n\n\nR.version.string\n\nUpdate R\n\nGo to CRAN and download the latest version for your operating system.\nRun the installer. (You don‚Äôt need to uninstall older versions‚ÄîR will install alongside them.)\n\nUpdate RStudio\n\nGo to Posit‚Äôs download page.\nDownload and install the newest version for your operating system.\n\nThat‚Äôs it! After updating, restart your computer to make sure RStudio finds the latest R.\n\n\n\n\n\n\nFor this lesson we will analyze a dataset of social media posts related to the Apple TV series Severance. The dataset was collected using Brandwatch (via UCSB Library subscription), and it includes posts from the two days following the finales of Season 1 (April 2022) and Season 2 (March 2025). The dataset contains over 5,800 posts stored in a CSV file.\nThe dataset is available for download from this link: Severance Dataset. You will need an active UCSB NetID and password to access the file (the same you use for your UCSB email).\nPlease download the file comments_variables.csv and save it in your RStudio project folder. (FIXME - SHOULD WE PROVIDE THE FOLDER WITH RENV?)\n\n\n\nThis level assumes a basic familiarity with R and RStudio. If you are new to R, we recommend you check out the Introduction to Data Analysis with R, particularly the Introduction to R and RStudio section.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Text Analysis with R",
    "section": "",
    "text": "These lessons are hands-on and are designed to be followed with R and RStudio open. Before starting, please ensure you have the following software installed:\n\nR: We recommend R version 4.3 or newer. Download from CRAN.\nRStudio: We recommend RStudio version 2023.12 or newer. Download from Posit‚Äôs website.\n\n\n\n\n\n\n\nNoteHow to update R and RStudio\n\n\n\n\n\nCheck your versions\n\nRStudio:\n\nOn Mac: Go to RStudio -&gt; About RStudio.\nOn Windows: Go to Help -&gt; About RStudio.\n\nR: In the R console, run:\n\n\nR.version.string\n\nUpdate R\n\nGo to CRAN and download the latest version for your operating system.\nRun the installer. (You don‚Äôt need to uninstall older versions‚ÄîR will install alongside them.)\n\nUpdate RStudio\n\nGo to Posit‚Äôs download page.\nDownload and install the newest version for your operating system.\n\nThat‚Äôs it! After updating, restart your computer to make sure RStudio finds the latest R.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#access-to-data",
    "href": "index.html#access-to-data",
    "title": "Text Analysis with R",
    "section": "",
    "text": "For this lesson we will analyze a dataset of social media posts related to the Apple TV series Severance. The dataset was collected using Brandwatch (via UCSB Library subscription), and it includes posts from the two days following the finales of Season 1 (April 2022) and Season 2 (March 2025). The dataset contains over 5,800 posts stored in a CSV file.\nThe dataset is available for download from this link: Severance Dataset. You will need an active UCSB NetID and password to access the file (the same you use for your UCSB email).\nPlease download the file comments_variables.csv and save it in your RStudio project folder. (FIXME - SHOULD WE PROVIDE THE FOLDER WITH RENV?)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#r-skill-level",
    "href": "index.html#r-skill-level",
    "title": "Text Analysis with R",
    "section": "",
    "text": "This level assumes a basic familiarity with R and RStudio. If you are new to R, we recommend you check out the Introduction to Data Analysis with R, particularly the Introduction to R and RStudio section.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/01_introduction.html",
    "href": "chapters/1.Preprocessing/01_introduction.html",
    "title": "Introduction to Text Preprocessing",
    "section": "",
    "text": "Text analysis is about digging into written words to find meaning, spotting patterns, tracking sentiment, or identifying key topics and themes. It‚Äôs essentially the ‚Äúwhat does this text tell us?‚Äù part of the process. Text analysis turns unstructured words into structured insights. Every day, countless emails, news articles, social media content, scientific literature, and reports are produced, hiding patterns, opinions, and signals that numbers alone can‚Äôt capture. By applying computational text analysis, we can uncover trends, measure public sentiment, compare perspectives, and extract meaning from massive collections of information that would otherwise feel overwhelming.\nText preprocessing is the set of steps used to clean, standardize, and structure raw text before it can be meaningfully analyzed. This may include removing punctuation, normalizing letter case, eliminating stop words, breaking text into tokens (words or sentences), and reducing words to their base forms through lemmatization. Preprocessing reduces noise and inconsistencies in the text, making it ready for computational analysis.\nNatural Language Processing (NLP) provides the computational methods that make text analysis possible. It is a branch of artificial intelligence that enables computers to understand, interpret, and generate human language. NLP techniques include part-of-speech tagging, named entity recognition, sentiment detection, topic modeling, and transforming words into numerical representations for analysis.\nThese three elements are co-dependent and work together. Preprocessing prepares messy text for analysis, NLP provides the computational tools to process and interpret the text, and text analysis applies these insights to answer meaningful questions. Without preprocessing, NLP models struggle with noise; without NLP, text analysis would be limited to surface-level counts; and without text analysis, NLP would have little purpose beyond technical processing. Together, they transform raw text into structured, actionable insights.From messy to analysis-ready text\nOf course, text isn‚Äôt quite as ‚Äúanalysis-ready‚Äù as numbers. Have you ever looked at raw text data and thought, where do I even start? That‚Äôs the challenge: before computers can process it meaningfully, text usually needs some cleaning and preparation. It‚Äôs extra work, but it‚Äôs also the foundation of any meaningful analysis. The exciting part is what happens next; once the text is shaped and structured, it can reveal insights you‚Äôd never notice just by skimming. And here‚Äôs the real advantage: computers can process enormous amounts of text not only faster but often more effectively than humans, allowing us to see patterns and connections that would otherwise stay hidden.",
    "crumbs": [
      "Introduction to Text Preprocessing"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/01_introduction.html#getting-things-started",
    "href": "chapters/1.Preprocessing/01_introduction.html#getting-things-started",
    "title": "Introduction to Text Preprocessing",
    "section": "Getting Things Started",
    "text": "Getting Things Started\nThe data we pulled for this exercise comes from real social media posts, meaning they are inherently messy, and we know that even before going in. Because it is derived from natural language, this kind of data is unstructured, often filled with inconsistencies and irregularities.\nBefore we can apply any meaningful analysis or modeling, it‚Äôs crucial to visually inspect the data to get a sense of what we‚Äôre working with. Eyeballing the raw text helps us identify common patterns, potential noise, and areas that will require careful preprocessing to ensure the downstream tasks are effective and reliable.\nTime to launch RStudio and our example! Open the worksheet.qmd. Let‚Äôs install the required packages (via the console) and load them (run the code chunk). Next, let‚Äôs inspect the comments.csv file and take a quick look at it! (FIXME: RENV AND PROJECT FOLDER?)\n# Inspecting the data\n\ncomments &lt;- read_csv(\"comments.csv\")\nhead(comments$text)\n\n\n\n\n\n\nNoteüí¨ Discussion\n\n\n\nWorking in pairs or trios, look briefly at the data and discuss the challenges that may arise when attempting to analyze this dataset on its current form. What could be potential areas of friction that could compromise the results?",
    "crumbs": [
      "Introduction to Text Preprocessing"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/06_conclusion.html",
    "href": "chapters/1.Preprocessing/06_conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "In this workshop, we navigated the challenges of preprocessing of unustrucutred social media data, highlighting how messy, inconsistent, and noisy real-world datasets can be. One key takeaway is the importance of thoroughly assessing the data in the context of your project goals before diving into processing and be mindful that the order of factors do influence the outcome.\nNot all cleaning or transformation steps are universally beneficial and decisions should be guided by what is meaningful for your analysis or model objectives. Emojis, for example, can convey sentiment, irony, or context that may be essential for analysis, so decisions on whether to remove, convert, or retain them should be goal-driven.\nSimilarly, numbers such as dates, prices, or statistics can carry meaningful information, but they can also introduce noise if misinterpreted or inconsistently formatted. Thoughtful handling of these elements ensures that preprocessing enhances the dataset‚Äôs usefulness rather than stripping away valuable signals.\nOverly aggressive text cleaning removes content that is vital to the context, meaning, or nuance of a text and can damage the performance of natural language processing (NLP) models. The specific steps that lead to this problem depend on the end goal of your NLP task.¬†\nWhile preprocessing is considered a key step, if performed incorrectly or poorly planned, it can do more harm than good to the analysis. In short, preprocessing is not merely a mechanical phase in the pipeline but a thoughtful design choice that shapes the quality, interpretability, and trustworthiness of all subsequent tasks.\nBy critically evaluating the data and aligning preprocessing strategies with the end goals, we can ensure that the cleaned dataset not only becomes more manageable but also more valuable for deriving actionable insights. Ultimately, thoughtful data assessment is just as important as the technical preprocessing steps themselves.\n\n\n\n\n\n\nTipü§ì Suggested Readings\n\n\n\nChai CP. Comparison of text preprocessing methods. Natural Language Engineering. 2023;29(3):509-553. https://doi.org/10.1017/S1351324922000213\nSiino, M., Tinnirello, I., & La Cascia, M. (2024). Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on Transformers and traditional classifiers. Information Systems, 121, 102342. https://doi.org/10.1016/j.is.2023.102342",
    "crumbs": [
      "Conclusion"
    ]
  },
  {
    "objectID": "chapters/1.Preprocessing/04_stopwords.html",
    "href": "chapters/1.Preprocessing/04_stopwords.html",
    "title": "Stop Words Removal",
    "section": "",
    "text": "Stop words are commonly occurring words that are usually filtered out during natural language processing, as they carry minimal semantic weight and are not as useful for feature extraction.\nExamples include articles (i.e., a, an, the), prepositions (e.g., in, on, at), conjunctions (and, but, or), and pronouns (they, she, he), but the list goes on. While they appear often in text, they usually don‚Äôt add significant meaning to a sentence or search query.\nBy ignoring stop words, search engines, databases, chatbots and virtual assistants can improve the speed of crawling and indexing and help deliver faster, more efficient results. Similar posistive effects applies to other NLP tasks and models performance, including sentiment analysis.\nFor this workshop, we will be using the package stopwords (more info) which is considered a ‚Äúon-stop stopping‚Äù for R users. For English language, the package relies on the Snowball list. But, before we turn to our worksheet to see how that process looks like and how it will apply to our data, let‚Äôs have a little challenge!\n\n\n\n\n\n\nNoteüß† Knowledge Check\n\n\n\nHow many stop words can you spot in each of the following sentences:\n\nThe cat was sitting on the mat near the window.\nShe is going to the store because she needs some milk.\nI will be there in the morning if it doesn‚Äôt rain.\nThey have been working on the project for several days.\nAlthough he was tired, he continued to walk until he reached the house.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n1. The cat was sitting on the mat near the window.\n2. She is going to the store because she needs some milk.\n3. I will be there in the morning, if it does not rain.\n4. They have been working on the project for several days.\n5. Although he was very tired, he continued to walk until he reached the house.\n\n\n\n\n\nNow, let‚Äôs return to the worksheet and see how we can put that into practice.\n# Remove stop words\n#FIXME: ADD NEW CODE\nAwesome! This step should bring our token count down to 72,309 (FIXME-CHECK) by removing filler and unnecessary words, getting our dataset almost ready for sentiment analysis. Next up is lemmatization.\n\n\n\n\n\n\nNoteüìë Suggested Reading\n\n\n\nCheck out this blog post for a summary of the history of stop words, discussion on its applications and some perspectives on developments in the age of AI.\nGaviraj, K. (2025, April 24). The origins of stop words. BytePlus. https://www.byteplus.com/en/topic/400391?title=the-origins-of-stop-words",
    "crumbs": [
      "Stop Words Removal"
    ]
  }
]