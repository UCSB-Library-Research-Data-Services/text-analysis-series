---
title: "Text Analysis - Workshop Code"
format: html
editor: visual
---

This notebook contains all the code from the Text Analysis workshop chapters, organized by topic. You can run each section independently or work through them in sequence.

**Topics covered:**

1.  Word Frequency Analysis
2.  N-grams and Word Sequences
3.  Collocations
4.  TF-IDF

## Setup

Install required libraries:

```{r}
install.packages(c("tidyverse", "tidytext", "ggwordcloud", "patchwork", "tidyr"))
```

Load required libraries:

```{r}
#| output: false
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(patchwork)
library(tidyr)
```

Load the data:

```{r}
# Adjust the path to match your data location
comments <- readr::read_csv("../data/clean/comments_preprocessed.csv")
head(comments)
```

# Part 1: Word Frequency Analysis

## Tokenization

Tokenizing the comments into words:

```{r}
tokens <- comments %>%
  unnest_tokens(word, comments)

head(tokens)
```

## Counting Word Frequencies

```{r}
word_counts <- tokens %>%
  count(word, sort = TRUE) 

head(word_counts)
```

## Visualizations

Bar plot of top 20 most common words:

```{r}
top_words <- word_counts %>%
  top_n(20)

ggplot(top_words, aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity") +
  labs(title = "Top 20 Most Common Words", x = "Words", y = "Frequency")
```

Word cloud:

```{r}
ggplot(word_counts %>% top_n(100), aes(label = word, size = n)) +
  geom_text_wordcloud() +
  theme_minimal() +
  coord_fixed(ratio = 1) +
  labs(title = "Word Cloud")
```

## Filtered Word Cloud

Define common words to filter out:

```{r}
common_words <- c("severance", "season", "appleTV", "apple", "tv", "show", "finale", "episode")

filtered_word_counts <- word_counts %>%
  filter(!word %in% common_words)
```

Create a filtered word cloud:

```{r}
ggplot(filtered_word_counts %>% top_n(100), aes(label = word, size = n)) +
  geom_text_wordcloud() +
  theme_minimal() +
  coord_fixed(ratio = 1) +
  labs(title = "Filtered Word Cloud")
```

## Words by Season

Filter and create word clouds by season:

```{r}
season_1_tokens <- tokens %>%
  filter(grepl("^s1", id)) %>%
  count(word, sort = TRUE) %>%
  filter(!word %in% common_words) %>%
  top_n(20)

season_2_tokens <- tokens %>%
  filter(grepl("^s2", id)) %>%
  count(word, sort = TRUE) %>%
  filter(!word %in% common_words) %>%
  top_n(20)
```

Side-by-side word clouds:

```{r}
p1 <- ggplot(season_1_tokens, aes(label = word, size = n)) +
  geom_text_wordcloud(color = "darkblue") +
  theme_minimal() +
  coord_fixed(ratio = 1) +
  labs(title = "Season 1")

p2 <- ggplot(season_2_tokens, aes(label = word, size = n)) +
  geom_text_wordcloud(color = "darkred") +
  theme_minimal() +
  coord_fixed(ratio = 1) +
  labs(title = "Season 2")

p1 + p2
```

## Unique Words by Season

Extract unique words for each season:

```{r}
top_50_s1 <- season_1_tokens %>%
  top_n(50) %>%
  pull(word)

top_50_s2 <- season_2_tokens %>%
  top_n(50) %>%
  pull(word)

unique_s1 <- setdiff(top_50_s1, top_50_s2)
unique_s2 <- setdiff(top_50_s2, top_50_s1)

unique_s1_tokens <- season_1_tokens %>%
  filter(word %in% unique_s1)

unique_s2_tokens <- season_2_tokens %>%
  filter(word %in% unique_s2)
```

Visualize unique words:

```{r}
p3 <- ggplot(unique_s1_tokens, aes(label = word, size = n)) +
  geom_text_wordcloud(color = "lightblue") +
  scale_size_area(max_size = 20) +
  theme_minimal() +
  coord_fixed(ratio = 1) +
  labs(title = "Unique Words - Season 1")

p4 <- ggplot(unique_s2_tokens, aes(label = word, size = n)) +
  geom_text_wordcloud(color = "lightcoral") +
  scale_size_area(max_size = 20) +
  theme_minimal() +
  coord_fixed(ratio = 1) +
  labs(title = "Unique Words - Season 2")

p3 + p4
```

# Part 2: N-grams and Word Sequences

## Creating N-grams

Creating bigrams (2-grams):

```{r}
ngrams <- comments %>%
  unnest_tokens(ngrams, comments, token = "ngrams", n = 2)

ngrams
```

Creating trigrams (3-grams):

```{r}
trigrams <- comments %>%
  unnest_tokens(ngrams, comments, token = "ngrams", n = 3)

trigrams
```

## Next Word Prediction

Bigram-based prediction function:

```{r}
next_word <- function(word, ngrams_df) {
  matches <- ngrams_df %>%
    separate(ngrams, into = c("w1", "w2"), sep = " ", remove = FALSE) %>%
    filter(w1 == word) %>%
    pull(w2)
  freq <- table(matches)
  nw <- max(freq)
  return(names(freq[freq == nw]))
}
```

Test the prediction:

```{r}
type_any_word <- "ben"
next_word(type_any_word, ngrams)
```

Run prediction in a loop:

```{r}
current_word <- "wow"
for (i in 1:5) {
  predicted_word <- next_word(current_word, ngrams)
  cat(current_word, "->", predicted_word, "\n")
  current_word <- predicted_word
}
```

## Trigram-based Prediction

```{r}
next_word_trigram <- function(phrase, trigrams_df) {
  words <- unlist(strsplit(phrase, " "))
  if (length(words) != 2) {
    stop("Please provide a two-word phrase.")
  }
  matches <- trigrams_df %>%
    separate(ngrams, into = c("w1", "w2", "w3"), sep = " ", remove = FALSE) %>%
    filter(w1 == words[1], w2 == words[2]) %>%
    pull(w3)
  freq <- table(matches)
  nw <- max(freq)
  return(names(freq[freq == nw]))
}
```

Test trigram prediction:

```{r}
type_any_phrase <- "best show"
next_word_trigram(type_any_phrase, trigrams)
```

# Part 3: Collocations

## Calculate Bigram Counts

Separate bigrams and count them:

```{r}
bigram_counts <- ngrams %>%
  separate(ngrams, into = c("word1", "word2"), sep = " ", remove = FALSE) %>%
  count(word1, word2, sort = TRUE)

head(bigram_counts, 10)
```

## Calculate PMI (Pointwise Mutual Information)

Calculate individual word frequencies:

```{r}
word_freqs <- comments %>%
  unnest_tokens(word, comments) %>%
  count(word, name = "word_count")

# Total counts
total_words <- sum(word_freqs$word_count)
total_bigrams <- sum(bigram_counts$n)
```

Calculate PMI for all bigrams:

```{r}
collocations <- bigram_counts %>%
  left_join(word_freqs, by = c("word1" = "word")) %>%
  rename(word1_count = word_count) %>%
  left_join(word_freqs, by = c("word2" = "word")) %>%
  rename(word2_count = word_count) %>%
  mutate(
    p_bigram = n / total_bigrams,
    p_word1 = word1_count / total_words,
    p_word2 = word2_count / total_words,
    pmi = log2(p_bigram / (p_word1 * p_word2))
  ) %>%
  arrange(desc(pmi))

head(collocations, 15)
```

## Visualize Collocations

```{r}
top_collocations <- collocations %>%
  head(20) %>%
  unite(bigram, word1, word2, sep = " ")

ggplot(top_collocations, aes(x = reorder(bigram, pmi), y = pmi)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 20 Collocations by PMI",
    x = "Bigram",
    y = "Pointwise Mutual Information"
  ) +
  theme_minimal()
```

## Collocation-based Prediction

Create a collocation-based predictor:

```{r}
next_word_collocation <- function(word, collocations_df, min_freq = 2) {
  candidates <- collocations_df %>%
    filter(word1 == word, n >= min_freq, pmi > 0) %>%
    arrange(desc(pmi))
  
  if (nrow(candidates) > 0) {
    return(candidates$word2[1])
  } else {
    return(NA)
  }
}
```

Compare prediction methods:

```{r}
test_word <- "mark"
freq_prediction <- next_word(test_word, ngrams)
colloc_prediction <- next_word_collocation(test_word, collocations)

cat("Frequency-based predictor:", test_word, "->", freq_prediction, "\n")
cat("Collocation-based predictor:", test_word, "->", colloc_prediction, "\n")
```

Run both predictors in loops:

```{r}
cat("=== Frequency-based predictor ===\n")
current_word <- "wow"
for (i in 1:10) {
  predicted_word <- next_word(current_word, ngrams)
  cat(current_word, "->", predicted_word, "\n")
  current_word <- predicted_word
}

cat("\n=== Collocation-based predictor ===\n")
current_word <- "wow"
for (i in 1:10) {
  predicted_word <- next_word_collocation(current_word, collocations)
  if (is.na(predicted_word)) {
    cat(current_word, "-> (no strong collocation found)\n")
    break
  }
  cat(current_word, "->", predicted_word, "\n")
  current_word <- predicted_word
}
```

# Part 4: TF-IDF

## Calculate TF-IDF by Season

Extract season and tokenize:

```{r}
comments_tfidf <- comments %>%
  mutate(season = str_extract(id, "s[12]")) %>%
  unnest_tokens(word, comments) %>%
  count(season, word, sort = TRUE)

head(comments_tfidf)
```

Apply TF-IDF calculation:

```{r}
comments_tfidf <- comments_tfidf %>%
  bind_tf_idf(word, season, n)

head(comments_tfidf, 15)
```

Top distinctive words per season:

```{r}
comments_tfidf %>%
  group_by(season) %>%
  slice_max(tf_idf, n = 10)
```

## Visualize TF-IDF

```{r}
top_tfidf_words <- comments_tfidf %>%
  group_by(season) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, tf_idf, season))

ggplot(top_tfidf_words, aes(tf_idf, word, fill = season)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~season, scales = "free") +
  scale_y_reordered() +
  labs(
    x = "TF-IDF",
    y = NULL,
    title = "Distinctive Vocabulary by Season"
  ) +
  theme_minimal()
```

## Compare TF-IDF to Raw Frequency

Top words by raw frequency for Season 1:

```{r}
top_freq_s1 <- comments %>%
  filter(grepl("^s1", id)) %>%
  unnest_tokens(word, comments) %>%
  count(word, sort = TRUE) %>%
  head(15)

print(top_freq_s1)
```

Top words by TF-IDF for Season 1:

```{r}
top_tfidf_s1 <- comments_tfidf %>%
  filter(season == "s1") %>%
  arrange(desc(tf_idf)) %>%
  head(15)

print(top_tfidf_s1 %>% select(word, n, tf_idf))
```